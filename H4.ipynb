{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML.H4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eastonco/CS437/blob/main/H4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsLGOMPRM8xK"
      },
      "source": [
        "# **Homework Assignment #4**\n",
        "\n",
        "Assigned: March 5, 2021\n",
        "\n",
        "Due: March 24, 2021\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of questions that require a short answer and one Python programming task. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        " (10 points) Consider the subset of (x,y) points shown below. These are actually a subset of the data points found in the sklearn diabetes dataset.\n",
        "\n",
        "x | y\n",
        "--- | ---\n",
        " 0.08 | 233\n",
        "-0.04 | 91\n",
        " 0.01 | 111\n",
        "-0.04 | 152\n",
        "-0.03 | 120\n",
        " 0.01 | 67\n",
        " 0.09 | 310\n",
        "-0.03 | 94\n",
        "-0.06 | 183\n",
        "-0.03 | 66\n",
        " 0.06 | 173\n",
        "-0.06 | 72\n",
        " 0.00 | 49\n",
        "-0.02 | 64\n",
        "-0.07 | 48\n",
        "\n",
        "\n",
        "For a candidate linear regressor with parameters $\\Theta_0 = 75.1$ and $\\Theta_1 = -0.001$, calculate the mean squared error with respect to the data points and perform one iteration of gradient descent , assuming $\\alpha = 0.01$. Show all of your work.\n",
        "\n",
        "> Mean squared Error = 8061.61506\n",
        "\n",
        "\n",
        "> Theta0 = 82.1645\n",
        "\n",
        "> Theta1 = 30.40203\n",
        "\n",
        "\n",
        "> Calculations: https://drive.google.com/file/d/1k4mOvJcBzF8uv3cV-895BDpCBY5X6EHy/view?usp=sharing\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(15 points) Explain what value is generated by the equation $h(x) = \\frac{1}{1+e^{-(\\Theta_0 + \\Theta_1 x)}}$ in logistic regression. What steps are taken to convert the linear regression algorithm into the classification-based logistic regression algorithm?\n",
        "\n",
        "> this equasion generates a logistic regression line that uses classificaiton rather than rather than generating a specficic values. To convert a linear regression alorithm into a classifciation based algorithim, instead of generating a y values, you determine a classification based on if the point is above or below the generated y value\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(12 points) Regularization is introduced in class as a numeric term that can be incorporated into a loss function. This is straightforward for algorithms such as neural networks that seek to optimize a numeric loss function. Here, explain possible ways regularization can be used to fine tune other types of algorithms, specifically decision trees and naive Bayes classifiers.\n",
        "\n",
        "> Regularization can be used to fine tune algorithms like decision trees and naive bays classifiers by helping prevent overfit which is common in these algorithms especially when you get to very high dementionalities\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "(80 points) The goal of this program is to give you detailed experience with naive Bayes classifiers as well as with text mining methods and libraries. In this program, you are tasked with writing a naive Bayes classifier to classifier phone messages as spam versus not spam (ham). To test your program, use the labeled data available from the UCI machine learning repository at https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection. The data is also available at https://drive.google.com/file/d/1nn2baOauApGbxrOeTb4l-30Qz1prPIS3/view?usp=sharing.\n",
        "\n",
        "You will need to read in each message and convert it to a set of features. Use the sklearn libraries to\n",
        "\n",
        "- remove punctuation\n",
        "- convert to lower case\n",
        "- create a bag of words vector that stores term frequency\n",
        "- filter out words from the stop list (stop_words)\n",
        "- include 1-grams and 2-grams\n",
        "- normalize frequences based on document length (tfidf)\n",
        "\n",
        "Report performance of your classifier on 3-fold cross validation in terms of accuracy and macro f1 score.\n",
        "\n",
        "Additionally, notice that the class distribution is imbalanced. There are 4,827 legitimate messages and 747 spam messages. Experiment with three alternative methods for addressing this issue and report impact of each method on performance.\n",
        "\n",
        "- Undersample the majority class so they are balanced.\n",
        "- Oversample the minority class so they are balanced.\n",
        "- Weight the data points based on class imbalance.\n",
        "\n",
        "> Code is below \n",
        "\n",
        "---\n",
        "\n",
        "#5.\n",
        "\n",
        "(10 points, part of semester project grade)\n",
        "\n",
        "Provide a proposal of your semester project. The proposal is typically 1-2 paragraphs long and should include the goal of your project, a description of the data you will use and any necessary data cleaning / preparation steps you will need to perform, the methods you will create or apply, and how you will evaluate the results. If you are working on a multi-person project team, clearly state the role of each person on the team in contributing to the project goal.\n",
        "\n",
        "> My semester project will be to create a knn or neural network algorithm to determine wether or not a stock will go up or down within a certain timeframe. I will be using stock data from yahoo finance or tdameritrade. Since this is all offical data, there will be little to no data cleaning or preparation. To get a baseline of the performance, I will backtest the algorithm on historical data then do live tests on current events to see how well it works on live data and on different timeframes. My strech goal is to instead of classifying the results as up/down, instead, quanitify the strength or expected move of the stock in a %"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMGAwMqEZtXN",
        "outputId": "1b6a7042-df00-41c2-b77c-f596b36f5a58"
      },
      "source": [
        "# https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction import stop_words\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pprint\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def readFile(file):\n",
        "    messageList = []\n",
        "    classifier = []\n",
        "    for line in file:\n",
        "        x = line.split(\"\\t\")\n",
        "        classifier.append(x[0])\n",
        "        messageList.append(x[1])\n",
        "    return messageList, classifier\n",
        "\n",
        "f = open('/content/gdrive/My Drive/437/SMSSpamCollection')\n",
        "messages, classifiers = readFile(f)\n",
        "\n",
        "# print first line of the first data file as a sample\n",
        "\"\"\"\n",
        "print('Sample file')\n",
        "print(\"\\n\".join(newsgroups.data[2].split(\"\\n\")[:10]))\n",
        "print(\"\\n\")\n",
        "print('data', newsgroups.data[2])\n",
        "\"\"\"\n",
        "\n",
        "# convert collection of documents to matrix of string (word) counts\n",
        "count_vect = CountVectorizer()\n",
        "\n",
        "# use regular expressions to convert text to tokens\n",
        "# split contractions, separate punctuation\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "count_vect.set_params(tokenizer=tokenizer.tokenize)\n",
        "\n",
        "# remove English stop words (try with and without this)\n",
        "print(stop_words.ENGLISH_STOP_WORDS)\n",
        "\n",
        "# include 1-grams and 2-grams\n",
        "count_vect.set_params(ngram_range=(1,2))\n",
        "\n",
        "# ignore terms that appear in >50% of the documents, try with and without this\n",
        "count_vect.set_params(max_df=0.5)\n",
        "\n",
        "# ignore terms that appear in only 1 document, try with and without this\n",
        "count_vect.set_params(min_df=2)\n",
        "\n",
        "# transform text to bag of words vector using parameters\n",
        "X_counts = count_vect.fit_transform(messages)\n",
        "\n",
        "# normalize counts based on document length\n",
        "# weight common words less (is, a, an, the)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
        "\n",
        "# train a naive Bayes classifier on data, multinomial for discrete features\n",
        "clf = MultinomialNB().fit(X_tfidf, classifiers)\n",
        "scores = cross_val_score(clf, X_tfidf, classifiers, cv=3, \\\n",
        "                         scoring='accuracy')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, classifiers, test_size = 0.33)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "#y_true_labels = list(map(lambda x: newsgroups.target_names[x], y_test))\n",
        "#y_pred_labels = list(map(lambda x: newsgroups.target_names[x], y_pred))\n",
        "\n",
        "print('Number of documents:', len(classifiers), ', 3-fold accuracy:', np.mean(scores), ', macro f1:', f1_score(y_test, y_pred, average='macro') )\n",
        "\n",
        "\n",
        "# confusion matrix, show first time\n",
        "print('Confusion matrix:')\n",
        "#pprint.pprint(newsgroups.target_names, width=200)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "frozenset({'de', 'please', 'nine', 'most', 'amount', 'hasnt', 'hers', 'but', 'out', 'beyond', 'six', 'whereafter', 'top', 'part', 'where', 'on', 'give', 'serious', 'latterly', 'must', 'made', 'before', 'see', 'someone', 'myself', 'once', 'first', 'else', 'nor', 'him', 'former', 'about', 'found', 'otherwise', 'are', 'thence', 'himself', 'will', 'thereafter', 'cant', 'ie', 'nobody', 'anything', 'sometime', 'cannot', 'everyone', 'couldnt', 'any', 'sometimes', 'those', 'ltd', 'could', 'done', 'or', 'move', 'we', 'amongst', 'nevertheless', 'put', 'name', 'has', 'below', 'under', 'toward', 'via', 'too', 'why', 'was', 'neither', 'not', 'no', 'whole', 'thru', 'detail', 'all', 'ours', 'co', 'a', 'herself', 'whither', 'somewhere', 'above', 'three', 'last', 'others', 'full', 'twelve', 'than', 'been', 'hereafter', 'between', 'anyhow', 'nowhere', 'thin', 'another', 'next', 'thereby', 'etc', 'somehow', 'its', 'afterwards', 'when', 'yours', 'meanwhile', 'thick', 'moreover', 'our', 'from', 'your', 'nothing', 'take', 'describe', 'mine', 'eleven', 'even', 'eg', 'hereby', 'anywhere', 'for', 'keep', 'her', 'thus', 'if', 'fill', 'by', 'well', 'throughout', 'of', 'me', 'that', 'now', 'you', 'sixty', 'hence', 'becoming', 'beside', 'own', 'go', 'very', 'with', 'other', 'and', 'always', 'do', 'again', 'she', 'around', 'off', 'to', 'may', 'get', 'anyway', 'it', 'within', 'perhaps', 'fifty', 'namely', 'much', 'among', 'upon', 'whereby', 'third', 'behind', 'alone', 'therein', 'although', 'becomes', 'more', 'until', 'many', 're', 'several', 'can', 'up', 'whereupon', 'five', 'have', 'less', 'mill', 'seem', 'everything', 'except', 'therefore', 'elsewhere', 'being', 'become', 'them', 'seeming', 'ever', 'least', 'beforehand', 'anyone', 'fire', 'seemed', 'both', 'so', 'towards', 'everywhere', 'they', 'besides', 'rather', 'yourselves', 'against', 'his', 'still', 'whenever', 'already', 'every', 'never', 'themselves', 'eight', 'bottom', 'few', 'interest', 'since', 'us', 'fifteen', 'ourselves', 'seems', 'as', 'whose', 'two', 'then', 'side', 'wherever', 'often', 'same', 'am', 'who', 'without', 'what', 'these', 'whence', 'amoungst', 'an', 'sincere', 'each', 'either', 'none', 'un', 'whether', 'how', 'inc', 'into', 'through', 'show', 'their', 'enough', 'wherein', 'over', 'yourself', 'con', 'find', 'at', 'empty', 'due', 'my', 'together', 'almost', 'herein', 'system', 'onto', 'cry', 'also', 'front', 'thereupon', 'however', 'such', 'only', 'were', 'call', 'there', 'across', 'while', 'latter', 'per', 'whom', 'noone', 'whereas', 'ten', 'this', 'whatever', 'yet', 'down', 'something', 'here', 'further', 'had', 'hundred', 'which', 'mostly', 'some', 'became', 'formerly', 'should', 'because', 'though', 'indeed', 'he', 'is', 'be', 'back', 'twenty', 'after', 'would', 'hereupon', 'bill', 'i', 'during', 'itself', 'in', 'whoever', 'might', 'along', 'four', 'the', 'forty', 'one'})\n",
            "Number of documents: 5574 , 3-fold accuracy: 0.9549695012558307 , macro f1: 0.885999588969886\n",
            "Confusion matrix:\n",
            "[[1597    0]\n",
            " [  82  161]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}