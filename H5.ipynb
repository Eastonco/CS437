{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML.H5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eastonco/CS437/blob/main/H5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF1vAj5wklz1"
      },
      "source": [
        "# **Homework Assignment #5**\n",
        "\n",
        "Assigned: March 24, 2021\n",
        "\n",
        "Due: April 12, 2021\n",
        "\n",
        "Connor Easton - 11557902\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of questions that require a short answer and one Python programming task. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for the your notebook as your homework submission.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        " (12 points) In most real world scenarios, data contain outliers. When using a support vector machine, outliers can be dealt with using a soft margin, specified in a slightly different optimization problem shown in Equation 7.38 in the text and called a soft-margin SVM.\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $\\zeta_i = 0$? Is this data point classified correctly?\n",
        "\n",
        "> point is touching hyperplane. Not classified correctly\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $0 < \\zeta_i \\leq 1$? Is this data point classified correctly?\n",
        "\n",
        "> point is very close to hyperplane. Not classified correctly\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $\\zeta_i > 1$? Is this data point classified correctly?\n",
        "\n",
        "> point has good margin to hyperplane.  classified correctly\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(12 points) Suppose the two-layer neural network shown below processes the input (0, 1, 1, 0). If the actual output should be 0.2, show step-by-step how the vector of weights *v* will be updated using backpropagation and $\\eta = 0.2$.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1mLkFgXA0drWp6nYL50n0BZv2Z13EA9CN)\n",
        "\n",
        "> https://drive.google.com/file/d/1OgMrFVzCdHPh7JNGHL5A8wep2HErgcBp/view?usp=sharing\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#3. \n",
        "\n",
        "(8 points) Under which of these conditions does an ensemble classifier perform best? There can be more than one right answer, explain all of your responses.\n",
        "\n",
        "- Low prediction correlation between base classifiers.\n",
        "- High prediction correlation between base classifiers.\n",
        "- Base classifiers have low variance.\n",
        "- Base classifiers have high bias.\n",
        "- Base classifiers have high variance.\n",
        "\n",
        "> Ensemble classifiers perfrom best when: there is a high prediciton correlation between the base classifier, and when the base classifiers have low variance. The first is because when all the base classifiers are in agreement, there is a high probability that they are all in agreement to the correct classificaiton. The second is because the low variance will lead to more consistent results of the base classififers.\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        " \n",
        "Test your neural network using the MNIST dataset. Information on loading and storing this handwritten-digit dataset can be found at https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html. Only consider digit classes '0' (which you can map onto value -1) and '1' (which you can map onto value 1). Train the network on a randomly-selected 2/3 of the data points and test on the remaining 1/3. You can report mean squared error or accuracy for the test data for a minimum of 10 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVTlQWKnJIs5",
        "outputId": "55eb6970-9954-47f9-dff9-c013ba0bb24e"
      },
      "source": [
        "from math import exp\n",
        "from random import random\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "  network = list()  # initialize weights to random number in [0..1]\n",
        "  hidden_layer = [{'weights':[random() for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
        "  network.append(hidden_layer)\n",
        "  output_layer = [{'weights':[random() for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
        "  network.append(output_layer)\n",
        "  return network\n",
        "\n",
        "\n",
        "def activate(weights, inputs):\n",
        "  activation = weights[-1]   # bias\n",
        "  for i in range(len(weights)-1):\n",
        "    activation += weights[i] * inputs[i]\n",
        "  return activation\n",
        "\n",
        "\n",
        "def transfer(activation): # sigmoid function\n",
        "  return 1.0 / (1.0 + exp(-activation))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def transfer(activation): # tanh function\n",
        "  return np.tanh(activation)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def forward_propagate(network, X, y):\n",
        "  inputs = X\n",
        "  for layer in network:\n",
        "    new_inputs = []\n",
        "    for node in layer:\n",
        "      activation = activate(node['weights'], X)\n",
        "      node['output'] = transfer(activation)\n",
        "      new_inputs.append(node['output']) # output of one node input to another\n",
        "    inputs = new_inputs\n",
        "  return inputs   # return output from last layer\n",
        "\n",
        "\n",
        "def transfer_derivative(output): # derivative of sigmoid function\n",
        "  return output * (1.0 - output)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def transfer_derivative(output): # derivative of tanh function\n",
        "  return 1.0 - np.tanh(output)**2\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def backward_propagate_error(network, expected):\n",
        "  for i in reversed(range(len(network))):\n",
        "    layer = network[i]\n",
        "    errors = []\n",
        "    if i != len(network)-1:\n",
        "      for j in range(len(layer)):\n",
        "        error = 0.0\n",
        "        for node in network[i+1]:\n",
        "          error += (node['weights'][j] * node['delta'])\n",
        "        errors.append(error)\n",
        "    else:\n",
        "      for j in range(len(layer)):\n",
        "        node = layer[j]\n",
        "        errors.append(expected[j] - node['output'])\n",
        "    for j in range(len(layer)):\n",
        "      node = layer[j]\n",
        "      node['delta'] = errors[j] * transfer_derivative(node['output'])\n",
        "\n",
        "\n",
        "def update_weights(network, x, y, eta):\n",
        "  for i in range(len(network)):\n",
        "    inputs = x\n",
        "    if i != 0:\n",
        "      inputs = [node['output'] for node in network[i-1]]\n",
        "    for node in network[i]:\n",
        "      for j in range(len(inputs)):\n",
        "         node['weights'][j] += eta * node['delta'] * inputs[j]\n",
        "      node['weights'][-1] += eta * node['delta']\n",
        "\n",
        "\n",
        "def train_network(network, X, y, eta, num_epochs, num_outputs):\n",
        "  expected = np.full((2), 0)\n",
        "  for epoch in range(num_epochs):\n",
        "    sum_error = 0\n",
        "    # There are two output nodes. The one corresponding to the correct label\n",
        "    # should output 1, the other should output 0.\n",
        "    for i in range(len(y)):\n",
        "      outputs = forward_propagate(network, X[i], y[i])\n",
        "      if y[i] == 0:\n",
        "        expected[0] = 1\n",
        "        expected[1] = 0\n",
        "      else:\n",
        "        expected[0] = 0\n",
        "        expected[1] = 1\n",
        "      sum_error += sum([(expected[i] - outputs[i])**2 for i in range(len(expected))])\n",
        "      backward_propagate_error(network, expected)\n",
        "      update_weights(network, X[i], y[i], eta)    \n",
        "    print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, eta, sum_error))\n",
        "\n",
        "\n",
        "def test_network(network, X, y, num_outputs):\n",
        "  expected = np.full((2), 0)\n",
        "  sum_error = 0\n",
        "  # There are two output nodes. The one corresponding to the correct label\n",
        "  # should output 1, the other should output 0.\n",
        "  for i in range(len(y)):\n",
        "    outputs = forward_propagate(network, X[i], y[i])\n",
        "    if y[i] == 0:\n",
        "      expected[0] = 1\n",
        "      expected[1] = 0\n",
        "    else:\n",
        "      expected[0] = 0\n",
        "      expected[1] = 1\n",
        "    sum_error += sum([(expected[i] - outputs[i])**2 for i in range(len(expected))])\n",
        "  print('mse of test data is', sum_error / float(len(y)))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Load data from https://www.openml.org/d/554\n",
        "  features, targets = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "  X = []\n",
        "  y = []\n",
        "  for i in range(len(targets)):\n",
        "    if targets[i] == '1' or targets[i] == '0':\n",
        "      X.append(features[i])\n",
        "      if targets[i] == '0':\n",
        "        y.append(-1)\n",
        "      else:\n",
        "        y.append(1)\n",
        "  n_inputs = len(X[0])\n",
        "  n_outputs = 2  # possible class values are '0' and '1'\n",
        "  # Create a network with 1 hidden layer containing 2 nodes\n",
        "  network = initialize_network(n_inputs, 2, n_outputs)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.67, test_size=0.33)\n",
        "  # train network for 10 epochs using learning rate of 0.1 \n",
        "  train_network(network, X_train, y_train, 0.5, 10, n_outputs)\n",
        "  for layer in network:\n",
        "    print('layer \\n', layer)\n",
        "  test_network(network, X_test, y_test, n_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">epoch=0, lrate=0.500, error=23.286\n",
            ">epoch=1, lrate=0.500, error=1.436\n",
            ">epoch=2, lrate=0.500, error=0.833\n",
            ">epoch=3, lrate=0.500, error=0.588\n",
            ">epoch=4, lrate=0.500, error=0.455\n",
            ">epoch=5, lrate=0.500, error=0.371\n",
            ">epoch=6, lrate=0.500, error=0.313\n",
            ">epoch=7, lrate=0.500, error=0.271\n",
            ">epoch=8, lrate=0.500, error=0.239\n",
            ">epoch=9, lrate=0.500, error=0.214\n",
            "layer \n",
            " [{'weights': [0.7029649542989028, 0.48836349218356, 0.4236293203879681, 0.9676770039797569, 0.8962045475314404, 0.8774715266330528, 0.24280808623053185, 0.5549958950935572, 0.7217222535635968, 0.1402445997285776, 0.5313866707559138, 0.4902582289400752, 0.48956625477498483, 0.3773565815891282, 0.13105105552035823, 0.06531900597730322, 0.75686489873331, 0.558658411206024, 0.9955285581730597, 0.9705857479755139, 0.4945859190507179, 0.9547111770972951, 0.3084972098672142, 0.39619887688450184, 0.6470264164877908, 0.7524159532946243, 0.8954209404463926, 0.7603955264601431, 0.7237992149409537, 0.8862116115253971, 0.32874758090589296, 0.21767564145138885, 0.8828369260554728, 0.05642655330256807, 0.9358804931696416, 0.5394145251835252, 0.8697474964945515, 0.9454519776406256, 0.4458474788187228, 0.2942024885900175, 0.47952614209542066, 0.9713532474906436, 0.4376098067932278, 0.18724125566467986, 0.5214694928120599, 0.8875277878209811, 0.15877599273715381, 0.023968674342172536, 0.13272540513465292, 0.9635061595242663, 0.38562965420943995, 0.5665052573786664, 0.07613946890335177, 0.8508631538482998, 0.7136155836065254, 0.5500344512360682, 0.052292736879219426, 0.5727550821057024, 0.7172504155307043, 0.5193654843523793, 0.053506557945421473, 0.06412480380447516, 0.6769880836174276, 0.5438481801490532, 0.5973596256188954, 0.7959923176906719, 0.5244548024566967, 0.5985304034878788, 0.349865956886516, 0.4167880985051018, 0.5717816536436945, 0.3962174978112748, 0.8933901018808078, 0.5460726990323155, 0.37135556949056947, 0.80094305641739, 0.6819498518224621, 0.7916686962388891, 0.7967127063445771, 0.8753216294744249, 0.6332376216335228, 0.4284276411895893, 0.11377358486208555, 0.8651942741543577, 0.9574127513445896, 0.022204492121689734, 0.8001604532142035, 0.4260948465027249, 0.7532800061350077, 0.8184866809458062, 0.15391558685416407, 0.016029980282052603, 0.46088141608136723, 0.9960999814824785, 0.973808439185183, 0.17124791260247096, 0.7361573145177183, 0.8265031777602755, 0.07820874140679379, 0.6918045841106685, 0.43151482897717464, 0.8386846734286473, 0.5822553198849771, 0.9903581811507969, 0.49441628307125307, 0.2945849449028506, 0.05945897747921014, 0.16112997117443684, 0.9719069570424487, 0.6648437882662599, 0.7500768830004688, 0.3171368700050162, 0.49656114070852286, 0.6944253394662172, 0.41239941103717437, 0.8108475211560914, 0.1436144745916007, 0.4990971504220032, 0.0619045048677076, 0.8447922843766414, 0.7288090619449628, 0.6404186968185837, 0.16386186313116569, 0.922310455667413, 0.6985439810438969, 0.7057349109474295, 0.4927148646853631, 0.03737778660648816, 0.8171194752153362, 0.5710386233732216, 0.7917855470609244, 0.8034721806651104, 0.21194700080705098, 0.21333650818835082, 0.8695179782393261, 0.6432046938380731, 0.08938228423266614, 0.7262758102126072, 0.4645296320726342, 0.7153166109623288, 0.0060560817022236035, 0.6343358311051412, 0.962338431951041, 0.8115226073218473, 0.743862661434229, 0.870174290159199, 0.00400304028030507, 0.5483860038645418, 0.7261131840469387, 0.910126183481563, 0.21622164662089682, 0.1293337521895631, 0.8029337296993226, 0.48572800983577413, 0.7404558461961362, 0.5749197933052251, 0.2399680929306538, 0.7281554745131964, 0.6575289604274265, 0.03412540524319574, 0.6125492944448002, 0.2575500762283426, 0.5391050845406533, 0.17641461649026013, 0.5698957724410839, 0.45224173282120783, 0.999760552774436, 0.6891181762279207, 0.17442839093884766, 0.9609276123645045, 0.39637786138775877, 0.8495202984174025, 0.9586343765804657, 0.7110638899157617, 0.4300234300497726, 0.9375972410814291, 0.34274212678396665, 0.4504914835140552, 0.04291329107654773, 0.22178345008525402, 0.06607130417665541, 0.40960308705136284, 0.9351943361749402, 0.41326168635217786, 0.5982326300228189, 0.6107864509683618, 0.19637193774547923, 0.7580845484577461, 0.12405270936621338, 0.9240703837417277, 0.372486449214971, 0.2324093405603168, 0.9059439521749201, 0.586185877372295, 0.8816171272343838, 0.7534791244115716, 0.9241576573170794, 0.5801387224106502, 0.26755119241555636, 0.627892442044377, 0.3132717076571281, 0.548005487401344, 0.3923392237514496, 0.011623372198386606, 0.016574180564586927, 0.07704342251853602, 0.8045813974281347, 0.05105904068160061, 0.052243993596893135, 0.42509916398458225, 0.3136859666166124, 0.2457405135583728, 0.4750362877619856, 0.5969156912943356, 0.9599291090997474, 0.880723926634484, 0.3636892611348913, 0.28782533308776714, 0.7264063399363647, 0.2881822418341996, 0.2181528028190457, 0.7749467773317152, 0.6410983707624214, 0.6894078616466969, 0.96070408345764, 0.24238970828373818, 0.5405546598634633, 0.24295916093660463, 0.9297830662254081, 0.8569191438347442, 0.5260131655350194, 0.5898650215072272, 0.9548487968749078, 0.5488350631644281, 0.6047490892555909, 0.5132400142506683, 0.027481709646849595, 0.9645296613459855, 0.7135216844371868, 0.834811588446888, 0.8488619251262673, 0.7935399818227061, 0.8733591586775487, 0.7029208804986236, 0.18864756008962957, 0.3306165364546535, 0.5977161112734455, 0.3869559681789617, 0.22796583525539071, 0.12230501884132505, 0.7551411014834966, 0.0042421332653550525, 0.16904342110985537, 0.24387646488067738, 0.29985296008630147, 0.19779257758759117, 0.9452266046991935, 0.31516558853088217, 0.016613704692458264, 0.5613666323326842, 0.37486530299309595, 0.9629068580091351, 0.8872704182012124, 0.10449905336216425, 0.2190982249135166, 0.7630510834707941, 0.6422328418009586, 0.3627075030365835, 0.983360559553282, 0.6824466343381083, 0.6509404588794843, 0.2603133651896149, 0.4684261067458402, 0.9128463447607402, 0.7872030964164892, 0.41819732167996715, 0.6496996565650991, 0.9181626363072315, 0.47852227134067493, 0.8962737024255724, 0.7059779559254339, 0.23260508073078245, 0.8262050993107308, 0.03578538626572236, 0.036019854708410515, 0.5287209097069886, 0.1434171329529822, 0.23659334025084044, 0.08082970661369271, 0.6990730088341183, 0.7711963122156805, 0.13194202413196288, 0.8025707590904135, 0.42725752995821453, 0.6450142382100152, 0.6317692589570446, 0.17324868746902577, 0.80504066893207, 0.2160955270040289, 0.5786451848443193, 0.15941544557916054, 0.8308944079563249, 0.5856629673916313, 0.7023868455130842, 0.4203211346257326, 0.873484362282952, 0.5640909553097054, 0.9462318675351551, 0.7258980832623317, 0.40062068527713846, 0.5268014673570158, 0.5543138404212791, 0.7913982434724603, 0.20401880799969985, 0.7669200396875012, 0.7404066771481022, 0.8558187815465225, 0.49463713594869785, 0.5531597186221087, 0.33511329327124717, 0.9892120628765254, 0.4743461814654907, 0.208278484835553, 0.49476414157268334, 0.4871810820051341, 0.5529674952660867, 0.644656941364995, 0.4456235721987999, 0.8346330878243278, 0.43889897420760515, 0.44726829469983065, 0.8170018502625982, 0.43015117090185895, 0.10243402270516133, 0.47470462433101845, 0.487731444376464, 0.5287826142379128, 0.4145079077383357, 0.21011606357303214, 0.2764063338143805, 0.8304880738696425, 0.6040714024261178, 0.0575330231902178, 0.5449912443220097, 0.9772780436727314, 0.4193599187317444, 0.9648693683157202, 0.8725728416085181, 0.39863509819738385, 0.9103364198598529, 0.9479080464363098, 0.6235429268364466, 0.3142029419557364, 0.5566084284394889, 0.08966419973963857, 0.6393326270330825, 0.07755738326847017, 0.1543284131546957, 0.32028201398724454, 0.16367932485658876, 0.8877749168857709, 0.757058491257265, 0.4506755297191012, 0.9610077259606198, 0.8347336010803103, 0.517300669595434, 0.812922279530268, 0.8378321201625163, 0.9166661811896283, 0.8311942652129004, 0.7390450090877768, 0.5990877689219198, 0.4700900578746128, 0.30519686502106713, 0.2164225420880097, 0.12302583733219719, 0.1465656613243057, 0.4652079226167999, 0.3125837806549543, 0.6771709677982686, 0.6026010977780739, 0.38687392049765634, 0.8879159220601649, 0.7324329007836947, 0.6905167573162214, 0.8376358037708287, 0.5820074715416633, 0.5104875095505338, 0.034684644531156605, 0.8616742264296445, 0.2693496779930953, 0.5185891824830116, 0.9437013103153753, 0.3331352862279978, 0.5246373682500136, 0.608529061987719, 0.7790877904223245, 0.9661027965983355, 0.5095534254345295, 0.034497131155404204, 0.06080234586763944, 0.15807053968854312, 0.9365849819156942, 0.19897731726826506, 0.7260360687966275, 0.5089556671911565, 0.7145771705359697, 0.2798451081909751, 0.9406421326487512, 0.35129453602623406, 0.3539320942211531, 0.5429299483075677, 0.7420754006504887, 0.7349203423428656, 0.2554426522834049, 0.08058723230984122, 0.8110899435456603, 0.8481265109746996, 0.22558494927003025, 0.671546997796666, 0.9144652276650606, 0.837846586012539, 0.11365970864601971, 0.035684645077711186, 0.49703130630315207, 0.11707483377952066, 0.09738346809101017, 0.8935728604073815, 0.6529415795182204, 0.14683901105743902, 0.5469961573096819, 0.10344277210808439, 0.5747956933915847, 0.09469019356218966, 0.4906477940574452, 0.6359396100637271, 0.447388376465513, 0.4931856674451811, 0.730432790700906, 0.6228062531313141, 0.9393517714930112, 0.28329568793730686, 0.1803402422313216, 0.103789112282067, 0.4115592438081189, 0.6871828115010193, 0.5120934015279571, 0.6831594371852986, 0.8091871479023943, 0.6989469285439084, 0.22138721348909496, 0.3959505661183996, 0.31286838545019324, 0.8239780061215946, 0.013887953679821652, 0.5126312654153675, 0.6165065411880949, 0.27233569236244337, 0.07333290316285146, 0.6719969532328665, 0.08117587842471641, 0.7037809597466045, 0.662357322691936, 0.5421139754051973, 0.5197099943354683, 0.5856484837046474, 0.6955230999770518, 0.12702068581959314, 0.608208502541073, 0.6847219065097911, 0.9529027124095241, 0.9254572792961898, 0.20591425204125158, 0.6320973292098839, 0.6502260136877672, 0.23675276660834066, 0.6965279273890621, 0.24197871273752336, 0.7828100822310966, 0.5199842037012937, 0.735744832405531, 0.597333861630389, 0.7021790533055642, 0.20011722455193737, 0.23116256575786787, 0.5902829524361324, 0.7951158301859654, 0.49504772623994997, 0.9078200744358815, 0.29686379074474845, 0.3493923007179497, 0.2313814319967833, 0.2563666453864596, 0.19871296568096142, 0.5538272850583507, 0.6390329281471752, 0.11038340854778905, 0.04133415587546552, 0.26868597813078576, 0.3414863071531815, 0.5061550182017331, 0.5465275259997837, 0.964916287404744, 0.39856778378088786, 0.9403716979422144, 0.24017430587267186, 0.5731850336068477, 0.8271957750848975, 0.04860395223668412, 0.8788289119419325, 0.9055823166322796, 0.5282050933032097, 0.8070562145478217, 0.16701472646304405, 0.30355257999018226, 0.18513328630006354, 0.9719265745983068, 0.6175595824978243, 0.9605842663637845, 0.014796758211796712, 0.8669320836214657, 0.8887035093444468, 0.2353877611295454, 0.999575462233849, 0.12598974954331732, 0.030544582916936736, 0.970458559056659, 0.31312002341642753, 0.4687242535871238, 0.578403026343301, 0.944502335137227, 0.8099681105595513, 0.2997998042349058, 0.0753493061242626, 0.36218460078059445, 0.16963448652420754, 0.9334832241659574, 0.07986278969821115, 0.43431543539049966, 0.6881091004108721, 0.3054120057324854, 0.7376544864599707, 0.24381524583518444, 0.7038113680672355, 0.6537159298574724, 0.8354969320801547, 0.18657017850484103, 0.5979112410933441, 0.582231936076842, 0.15084595541391654, 0.37628996417627225, 0.948331367047947, 0.0251735637322007, 0.3291241981945954, 0.9217629805114053, 0.5186213835594923, 0.9663284074854976, 0.18468299740104532, 0.17831885069025144, 0.936859035704348, 0.07681572171134166, 0.6508416201975092, 0.9752152927151869, 0.5357545855222815, 0.4396583966050732, 0.715821210676203, 0.7295582107853692, 0.7854974798484375, 0.355030829226727, 0.977936253261313, 0.9944067399844917, 0.18562974305134172, 0.14319422363619694, 0.6049725214001584, 0.2503618337106437, 0.3212967803337464, 0.46336017445384425, 0.8854590010377139, 0.5279462535922482, 0.1616534401377877, 0.6175640631481215, 0.7577066187339166, 0.11439817816104114, 0.11080218691382127, 0.8174211420014391, 0.7433505554574902, 0.20471257307299096, 0.5307436922191293, 0.7390875373721143, 0.4291671120257463, 0.1940417400949902, 0.018368796995918868, 0.3472446981046319, 0.3302737296876711, 0.38051966383229807, 0.9657385878026328, 0.6694327510654705, 0.15662265892168004, 0.8447730409880634, 0.23619954387809117, 0.8761018352193563, 0.15179973200709262, 0.14801817090536762, 0.988859752117261, 0.839517364076565, 0.8377106367295508, 0.7734705748574152, 0.1999992820508364, 0.6737967810822753, 0.08053493892965335, 0.9471406489482367, 0.3833330401236319, 0.46870060397777935, 0.08435322918227606, 0.20890004421258912, 0.208925341362742, 0.6992435886929538, 0.6909280199593347, 0.41495470339682716, 0.42500301004970886, 0.16008454255191507, 0.39547710140254566, 0.012987791013413852, 0.10440865490782492, 0.8965379041241145, 0.4930503247025615, 0.4422628383905034, 0.6852865852784118, 0.9938893180352443, 0.8144826443423963, 0.7930883361785618, 0.24659576738863986, 0.3988883905416297, 0.5874509501217776, 0.3367542539824435, 0.09882001746731983, 0.2431436397804998, 0.8196238520157949, 0.34598867154521296, 0.5274206268283583, 0.05773663567682075, 0.9180717335391015, 0.2112544446337551, 0.4227675221676255, 0.06681513714488352, 0.9059101191510914, 0.7041453438206368, 0.8358886833272423, 0.47185412096548707, 0.8792820114078523, 0.9036495645161952, 0.6918172986385227, 0.14745666502403976, 0.09975724197880342, 0.4880294066651921, 0.8589417066796058, 0.6418897769285916, 0.6358960148264832, 0.8217982901848537, 0.12514919475590758, 0.44772464467037576, 0.34431543612888527, 0.8302618837567615, 0.01593203255284159, 0.5443711411818367, 0.1153841329959453, 0.15465945414488724, 0.18516383618895327, 0.3355077688230085, 0.3124838794796374, 0.6708934002895822, 0.8397835541768176, 0.8012120547622136, 0.18592864392856978, 0.7572299607235801, 0.2746648618348083, 0.9270691947159322, 0.25488856585632946, 0.5140788570662935, 0.7359249428021899, 0.21225105303269565, 0.24767760098080904, 0.4947127334155662, 0.9691081627466391, 0.060112676307868984, 0.2955206000485867, 0.4302480079912828, 0.566603499135817, 0.7400272655417526, 0.29051505988001214, 0.08966627134745986, 0.47162218287363167, 0.34689377720614833, 0.8419616916300914, 0.5095474876962329, 0.9137731656563913, 0.8701886528060269, 0.4128351535965351, 0.9501792148570493, 0.9099347598496422, 0.9113541533305222, 0.6957422223979316, 0.47422178526409653, 0.6273044946028175, 0.09159502896084815, 0.8827693390941083, 0.28016392273318147, 0.6193772103691793, 0.3574742376388296, 0.7633403771313558, 0.37043427622559044, 0.06287146493151397, 0.8087810435650163, 0.026575031558484064, 0.526404858350497, 0.31102931529838573, 0.2517936859823169, 0.5532732406630766, 0.7378442703570741, 0.48243511662521965, 0.8004206133121906, 0.7068418291256579, 0.12334630923023338, 0.688011698299024, 0.28456720360793963, 0.4585445368090243, 0.6143969018233572, 0.7751197536834565, 0.26052290593779925, 0.3554157209999971, 0.8797116856759798, 0.3199521522473858, 0.3565141767562987, 0.5057542805476587, 0.8131051717699039, 0.8798271988796983, 0.2240929001858427, 0.5483119929255257, 0.5416036574591028, 0.3101346765015016, 0.822764898550154, 0.6777116217631508, 0.7353899620205979, 0.9853584602966109, 0.03588205040890202, 0.19951346003275183, 0.02919859127931379, 0.991575560250679, 0.626241598868731, 0.6594662763492927, 0.5601165783883534, 0.9033318225927212, 0.24530011995079504, 0.7576583295695797, 0.9826792692624098, 0.6525411219134918, 0.4774826382969891, 0.291488604864078, 0.014506318018855713, 0.5010599992723815, 0.9977440311949602, 0.7782410861534141, 0.3459488923611712, 0.9329104867892271, 0.615545610182138, 0.7158216324464965, 0.02367982684711456, 0.7890111914195784, 0.2691063682634809, 0.5115365518818704, 0.06850193450197728, 0.13575670627829262, 0.8028250329849222, 0.6242749287114766, 0.048881948753929705, 0.3145649331206182, 0.8060914996657756, 0.523567406089599, 0.9527468644035577, 0.21459413013842066, 0.5291003459220516, 0.9438417728993866, 0.36854373487095793, 0.405765215129928, 0.4041994832397333], 'output': 1.0, 'delta': 0.0}, {'weights': [0.9224520775250248, 0.14398853898350372, 0.49450382811432925, 0.25004410605452443, 0.4488276493801644, 0.6605710721486815, 0.9108671989273103, 0.2340146628160471, 0.9216507216767027, 0.4251733149892294, 0.6412648712509014, 0.566669991260305, 0.01016298450588038, 0.7618878722931868, 0.02685617814559038, 0.8215518133141821, 0.03831775414168126, 0.02383287691200031, 0.11577291330785477, 0.848850261619379, 0.5499315833781206, 0.29908649971916845, 0.32188719468988836, 0.9485892587833564, 0.5797278331486644, 0.8041884222017958, 0.8430058763528243, 0.44103232335053755, 0.53409071761172, 0.2987654212479761, 0.13169863541801885, 0.7618305962653591, 0.7768055291195102, 0.7181801437541715, 0.4876425638236336, 0.6760635663395342, 0.31262110527539977, 0.8263598266251007, 0.5985582228074676, 0.07578942817337353, 0.5079734837094357, 0.8499232606666233, 0.7323826687489532, 0.0977070122253173, 0.749969131023945, 0.13165721861885282, 0.7017051385393807, 0.40138712565657053, 0.7493532214807908, 0.8150512970520956, 0.20360880956574956, 0.8697027514284176, 0.7653156588257408, 0.8251256205278964, 0.07693403688058587, 0.6064978000162731, 0.5699717774409989, 0.7262764357227929, 0.6457911891803064, 0.9440317127323571, 0.7823272419073724, 0.7783931687507262, 0.16170331955594663, 0.28792108835912933, 0.668662051476255, 0.772869314181741, 0.20104136225143743, 0.5529893887373911, 0.2882927505651425, 0.47816974617602526, 0.1058595030010937, 0.6746088274715377, 0.4543845146654666, 0.90130897942289, 0.21326210553816438, 0.6926978838912555, 0.6391853242400689, 0.4167175439214331, 0.7401966394134496, 0.8266530380087131, 0.9585808390778052, 0.1163789999523629, 0.6329866592658197, 0.06019653055284668, 0.3959346296641558, 0.7059925126737231, 0.327223503260369, 0.9222547345211926, 0.5659631283821988, 0.6355215424097589, 0.3321068043653743, 0.06141223531517925, 0.39354546607226537, 0.4875827980322579, 0.6872842154308927, 0.09965286032320264, 0.4087340438033127, 0.423515651259073, 0.3081742118548334, 0.7112999719743549, 0.09215044255755478, 0.6131854553062472, 0.44304157877561823, 0.61585096629492, 0.9186371559381714, 0.9752677035372139, 0.07502319303906824, 0.5191698302222214, 0.3179107579829724, 0.0466084495517225, 0.727608678644402, 0.3855741359756705, 0.4236415499429168, 0.8829034944916834, 0.33882731683491896, 0.020987278032757795, 0.0019670623949394894, 0.36609575221521706, 0.24316654918075187, 0.8138284962259473, 0.8933218237735517, 0.43409603869277436, 0.5644392639156561, 0.10217387349143392, 0.49745653641998966, 0.2837995088454415, 0.9892843389245037, 0.2381694613580302, 0.21856682826591578, 0.7020811849150806, 0.9740864561357484, 0.8570166295077166, 0.1640117088781814, 0.2378467202041189, 0.6210271921913331, 0.9197581754305337, 0.9664762861401572, 0.6582572919454173, 0.008446862200708005, 0.03370807649758201, 0.006325376586038378, 0.15120983055814652, 0.5541122902080607, 0.4907861371567106, 0.9074539509979959, 0.32337794666274666, 0.3141601869653847, 0.42808249890613115, 0.8712033607945509, 0.31204323926653876, 0.5106212759841233, 0.7954476284583596, 0.9448670301332577, 0.7153420836152892, 0.5436552834013716, 0.22968113737019769, 0.3394316915098151, 0.22066002564359544, 0.16010068518867127, 0.1905484158551609, 0.08419504844508041, 0.05841059911103463, 0.10871138700762695, 0.6562240036220128, 0.06275687048564482, 0.8327651847938823, 0.43947012615798275, 0.6267391909095614, 0.4339352600660302, 0.3554315186240946, 0.19598204135166453, 0.4438875396684008, 0.4360104978786413, 0.6793120679412279, 0.7128289966423718, 0.0960685076604677, 0.27428550369915616, 0.29790550570066987, 0.8652298191567198, 0.09193058241994156, 0.8939034622751961, 0.6487333052011439, 0.15024803091248873, 0.6569144977242487, 0.8346706799769729, 0.24219454390458617, 0.5864882921899334, 0.45905900061517024, 0.9744599810548845, 0.7592979770342132, 0.10567261695748698, 0.8185462067613001, 0.06202965943801442, 0.8115279479285139, 0.9342716459669123, 0.08268437537811202, 0.6364037648443829, 0.08294870406159405, 0.49669723434668844, 0.916374101200229, 0.8059589665782609, 0.6696434988576656, 0.557773402123666, 0.22958830673608632, 0.07047218520664456, 0.3365673444355125, 0.09926169568482868, 0.21162293000524268, 0.10010733457297982, 0.885362372897315, 0.006115433457982822, 0.3138144678852195, 0.6710916366551617, 0.9086523265851764, 0.7260127830743817, 0.5562646061988586, 0.48099219867239795, 0.9713853503777211, 0.7298817579783803, 0.7811068186997705, 0.1264537633324746, 0.649901502181421, 0.8079862847850444, 0.4751126629575595, 0.3553830715765047, 0.06689748082972247, 0.7482752812794404, 0.8797257481113827, 0.13566108642652197, 0.49746823477074853, 0.46935538477932104, 0.5768344538520366, 0.1732134188271418, 0.537623500851235, 0.27682828113764013, 0.6410851802184848, 0.15719795790670643, 0.06088595924173035, 0.46722843290247607, 0.37616098279663124, 0.6912553723712883, 0.9284664610134372, 0.10642028300672846, 0.2005081669255211, 0.025936243879381204, 0.46432318239078174, 0.8267895077779749, 0.8643810565099295, 0.3059647424889048, 0.5908673573123965, 0.46358871143101343, 0.057263609840485286, 0.343323346385061, 0.6514960431199628, 0.9306250941131052, 0.43749456713565826, 0.6899091267008985, 0.10187909592059086, 0.4996484006396871, 0.4332435978493927, 0.6332827592627167, 0.00942298485869797, 0.8052305196437342, 0.5569143584927192, 0.22553183607798555, 0.0004649522829875963, 0.7490696702279704, 0.3201740914577441, 0.33703637390981256, 0.6343558245412264, 0.6551673082540465, 0.6968210055673412, 0.40303560720926745, 0.1182888610016044, 0.03147916318258981, 0.6699955426258773, 0.44068347688765264, 0.06965223179319924, 0.9538538879154582, 0.5915090569200915, 0.7284709797427932, 0.35457234292095663, 0.7691847219425451, 0.620322109773034, 0.643430833254059, 0.958730723158542, 0.799136647247186, 0.09185363784570943, 0.7398524257963961, 0.6682338416738264, 0.1407267846791136, 0.36830560775535837, 0.7984323925830876, 0.011139563986136247, 0.20801575640729775, 0.8628202860395595, 0.2650167942286158, 0.04103850071170456, 0.5012134778843147, 0.035524397752492054, 0.4900796186525508, 0.7709165233101177, 0.028640276452347457, 0.7483396768960728, 0.6948906119285104, 0.7058619746998605, 0.2964442636421669, 0.9599525500240323, 0.3785442656058404, 0.6739095811718505, 0.341318264227922, 0.73766722869888, 0.5463330872911258, 0.6802732897676139, 0.6217204568948514, 0.7441596917218453, 0.36986133407894706, 0.4710326058208981, 0.5927115135428541, 0.14441104503906776, 0.14980222401807153, 0.6171948251516888, 0.8918374152497548, 0.583345120919927, 0.09120901151561323, 0.8394913005309252, 0.8709255436280179, 0.6685610249379621, 0.3073953330139453, 0.12415051890162576, 0.5552740483035958, 0.38045132873585075, 0.5014434530816488, 0.8976081432398085, 0.6279723235507547, 0.10152685921349436, 0.30819675992065165, 0.922015717845299, 0.8111392031705212, 0.26616457426408147, 0.6443642513807875, 0.6024087793765797, 0.10867555501780646, 0.9598743470205907, 0.8403516043559943, 0.5018006043789107, 0.3841381356566451, 0.7145389183593052, 0.5636109086226869, 0.6350601329486995, 0.7359956363383244, 0.48151824655050957, 0.773896914314084, 0.9729929432338196, 0.6346284974314578, 0.01305964756520106, 0.4355280029329791, 0.5263803785223262, 0.2852827898086623, 0.8701255281692851, 0.12861637235607182, 0.8305048001059139, 0.5602598325099452, 0.20430209220985918, 0.6121542561667475, 0.8901674878736484, 0.05205965133600221, 0.13228671585252738, 0.7031851477289739, 0.31800710298192514, 0.5257709573687572, 0.4302534339551697, 0.28900877801161995, 0.7452010418961389, 0.7625629840462232, 0.6037069998166525, 0.5618002164003035, 0.8169041295025327, 0.55573151920479, 0.6594191302555904, 0.8000187415668941, 0.6969798712877255, 0.7681455437908776, 0.5593808307580429, 0.1750255401818761, 0.3834490911203834, 0.9071299909631525, 0.1966124082771542, 0.4523932527633804, 0.8609202615888087, 0.6317401325048576, 0.6187436469710005, 0.14276908204789684, 0.9445134817484468, 0.3380902163361993, 0.48649210815460864, 0.8430277009694942, 0.397893062254434, 0.18548700714397726, 0.4912541959984015, 0.08316402601704975, 0.6709577558419391, 0.5351957858964111, 0.5807491226548862, 0.20580708525408586, 0.8795418226435372, 0.009983518330621277, 0.2526014542216173, 0.645526736285552, 0.35298100998526205, 0.5011296165249204, 0.9130537118723864, 0.3304499288245333, 0.7760076680967237, 0.5697310947923991, 0.5692862026625758, 0.7505890472963969, 0.6674329217253111, 0.14230302651526006, 0.8473594864181977, 0.47378399416060835, 0.6526798701587928, 0.17965261472081817, 0.17012271830872172, 0.41934501641505817, 0.4432321623967458, 0.6563153195705442, 0.5217692628046974, 0.5141867672394819, 0.9470764070749972, 0.7532872984563694, 0.6886769003047756, 0.8564614926870749, 0.9734481639821205, 0.32204468623148297, 0.061495437109324924, 0.7468541078749087, 0.4276944023664223, 0.851379824806169, 0.7002170843161758, 0.3201163125571238, 0.6430095485488485, 0.4539352038365635, 0.5938486871030728, 0.09932056411703405, 0.5371362491915254, 0.1545148086133441, 0.9229817275327448, 0.027040998224090118, 0.28922711708049353, 0.8645393372126968, 0.22458940941845806, 0.6596922514385657, 0.3075944685808901, 0.2642318561453624, 0.6698793634827004, 0.8599764595985141, 0.9363999745733362, 0.3197517737928731, 0.8762347090160871, 0.09772941198814489, 0.2359102508487012, 0.8006038227129391, 0.13079490181535514, 0.9946497085321394, 0.4235860068294641, 0.5944376739150689, 0.5687542003789577, 0.6517001736672893, 0.49144731856059387, 0.07535140847068533, 0.5999671169932008, 0.7272144705021386, 0.06402739372669608, 0.9105370160433485, 0.9216871345556906, 0.9424119791162636, 0.8234691471729745, 0.3837463098270891, 0.9613889779088998, 0.18033113056722294, 0.7203894534936317, 0.4827312508886299, 0.1280005770162541, 0.4505310597320058, 0.8884579547690478, 0.877562691704333, 0.591933112981052, 0.8741075252411938, 0.7048187243412033, 0.8294305975357414, 0.6988597531169387, 0.3720011207191992, 0.5184855850986386, 0.2647628906593765, 0.23996773477346167, 0.1358990389082838, 0.8330903482505119, 0.12024261754465915, 0.937549026513444, 0.724843894701888, 0.955977599848837, 0.3977374087360981, 0.04121121098549474, 0.28905586526709015, 0.51245331290314, 0.744136825621148, 0.8499588336743634, 0.32978006922691827, 0.6633998847653414, 0.7768511916184646, 0.04377544301140057, 0.8999270260467648, 0.6741433708566241, 0.8217089824362241, 0.2866224541568402, 0.6432951296604156, 0.39576109845477225, 0.9049894426076623, 0.5429336966974384, 0.29002898308583036, 0.6429506765518013, 0.03958929473133077, 0.8526057914409664, 0.7338170316931428, 0.49540374359923534, 0.8368269509634157, 0.18492194347243684, 0.39042554177187394, 0.8735050199434373, 0.6795793338396181, 0.46541750404337723, 0.9708708338504761, 0.17037998469056714, 0.7296274915445462, 0.6593987807513397, 0.9257030899563392, 0.2024528057083026, 0.07993815903254908, 0.06208092862148118, 0.9015582470699405, 0.4405613809637554, 0.37935973669801193, 0.8815390836820001, 0.8733224722269168, 0.8851781698319697, 0.20539152529355753, 0.7848069578454433, 0.09782240265968467, 0.5964842076298923, 0.8823140211780394, 0.6583465004935285, 0.46257940264046715, 0.21391508046669505, 0.9189898012647464, 0.12441880569860286, 0.21044924728480074, 0.6359348204879051, 0.0295373936785438, 0.8390002696860166, 0.0074634763550158745, 0.831890067087917, 0.9929314025771885, 0.2400359130386348, 0.4146550807866224, 0.28294227474129696, 0.582741383264525, 0.9423315918399315, 0.6001211290281149, 0.5894559401503792, 0.4761787479639792, 0.6877267788585675, 0.692183018142064, 0.5399942059119253, 0.27326043630351726, 0.01811457079524781, 0.5402751875476345, 0.45588610483627356, 0.3464046791440242, 0.2774777660512546, 0.6453549465626983, 0.11647649178253505, 0.7649531760490452, 0.7152742770327153, 0.0693704968284391, 0.6359226586005228, 0.6853540780654223, 0.5041935142717158, 0.2697866886255512, 0.7609026778838401, 0.26841803202247927, 0.4463565287016592, 0.6986071721467898, 0.23495370786294056, 0.48417841267625616, 0.36181165351090816, 0.24023130364850465, 0.4233483780715679, 0.559976820548324, 0.1043706728102598, 0.6695119165487732, 0.42512778320134303, 0.3885024339775669, 0.9294187137237325, 0.37395031131180356, 0.23449272034439106, 0.05378031759732649, 0.29056099135291924, 0.23177631728862902, 0.7931650327651051, 0.6224741702760033, 0.3652674461839932, 0.7565718168726181, 0.1703356351050126, 0.5106284032535608, 0.41671736558889316, 0.9071704295107634, 0.36613411564052945, 0.6074252712323058, 0.834256660876344, 0.025107349129584122, 0.6374812618166061, 0.2585150419696708, 0.056078525995718054, 0.4557381812236653, 0.20256399601389508, 0.22923174891409526, 0.8664440841523668, 0.22930323932650276, 0.6077785707868903, 0.7781437232040844, 0.6612547264071184, 0.13961864427969672, 0.12028377856499084, 0.30743004936782714, 0.41198501091216433, 0.6954005351830941, 0.7783331899355369, 0.10236784452448511, 0.2475896356260412, 0.15110378985517214, 0.9732266808557998, 0.6711420669220806, 0.27744665121225887, 0.22158373292821087, 0.09616840420586503, 0.07817905607284392, 0.314314462812039, 0.28189414260637746, 0.2494315627367789, 0.15149977573686146, 0.45753821928062355, 0.7112069202936363, 0.05121581623109639, 0.40604205483664946, 0.11843432587733038, 0.5137703333538167, 0.8514289242131792, 0.3457247620210412, 0.15622082209024712, 0.646177849232466, 0.3752758051663463, 0.09599318757386988, 0.16633231910243607, 0.13497239018674423, 0.4297130498813083, 0.5900214496228914, 0.20137966457499668, 0.11078913105682164, 0.11119732869829235, 0.7829850510405403, 0.574246146472351, 0.18219736590979518, 0.2466725440798192, 0.9161908701164235, 0.28548945984634677, 0.8836155033610463, 0.6186464672855926, 0.07189110771593021, 0.8462057960510342, 0.042516470859974786, 0.12841063258387675, 0.9948032165673787, 0.9713862932238677, 0.6043253449721607, 0.811331064177783, 0.8672710169788292, 0.5109214067419577, 0.09590911474752428, 0.3658396058813955, 0.3328244753860943, 0.719916009376772, 0.7655808235326811, 0.7736995932678322, 0.7249403953838504, 0.39809597407909236, 0.8775183157497274, 0.30476651226762286, 0.7089322073700823, 0.47684009701857233, 0.7171471240413105, 0.35515201424398246, 0.0254542100885492, 0.8502433504741757, 0.8760538884419631, 0.23431340244438525, 0.9933625164792044, 0.7051340511021993, 0.21018547391626174, 0.251628449536508, 0.33593073310837573, 0.7388617129501847, 0.2711331283148468, 0.3403580169691379, 0.5555781928898297, 0.42119447479128724, 0.3987248263008534, 0.9435408736362259, 0.20087395693077648, 0.32838979021335724, 0.06552389200490627, 0.19208110571457615, 0.06458488336619328, 0.04377054214869358, 0.12493387365618713, 0.7076688877943924, 0.273067743440054, 0.2740406635705107, 0.948848099254793, 0.2518381884307368, 0.6120056514240536, 0.9445208243135127, 0.22999003951212038, 0.4026151407907702, 0.6375389913806461, 0.9912440481436143, 0.24474442058914836, 0.0328796992183924, 0.20660088527135134, 0.6480009451019979, 0.7657957692334536, 0.3050687878764704, 0.9137074559471648, 0.9932409471001165, 0.1547993463242574, 0.5304638250549523, 0.8306041615623565, 0.25045913941051245, 0.8686668916368907, 0.4241559621993167, 0.5798136822207723, 0.5357530484972526, 0.12536472319053482, 0.7493316390136437, 0.4121029278055518, 0.771566498473188, 0.07855743579050989, 0.5734755475290104, 0.782259429492096, 0.18910052868070892, 0.1573438479273167, 0.33189513023647277, 0.34634726256767834, 0.20809764804586028, 0.972468491464288, 0.12409781758983152, 0.01772727849003597, 0.6778585118233288, 0.9910757909521815, 0.4762616955722373, 0.8569851277895403, 0.9744317445710854, 0.09962406630708398, 0.35272398948439054, 0.8333298821619292, 0.9644043216453072, 0.9387670204382548, 0.8746788833323802, 0.7883935348721395, 0.7444349310714832, 0.00432445355671951, 0.8598311855949173, 0.5967591837851688, 0.5359997413877597, 0.763425911776195], 'output': 1.0, 'delta': 0.0}]\n",
            "layer \n",
            " [{'weights': [-5.31115921152234, -5.182109713290363, -5.741846787924391], 'output': 0.003198589317872317, 'delta': -1.0198248941460432e-05}, {'weights': [5.488412189912886, 6.255218985457181, 5.741901935485748], 'output': 0.9968015865092823, 'delta': 1.0197129571010814e-05}]\n",
            "mse of test data is 2.0460614497332288e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}