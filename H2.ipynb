{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML.H2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eastonco/CS437/blob/main/H2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Ly48iQZeu5"
      },
      "source": [
        "# **Homework Assignment #2**\n",
        "\n",
        "Assigned: February 1, 2021\n",
        "\n",
        "Due: February 19, 2021\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of questions that require a short answer and one Python programming task. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        "(10 points) Consider training set accuracy and test set accuracy curves\n",
        "plotted below as a function of the number of nodes in a decision tree.\n",
        "While this graph plots accuracy, we can also compute error as 1.0 - accuracy.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1ScPyMBFemm6dbgu1saUqSV3dJdUlwIdd)\n",
        "\n",
        "Can you suggest a way to determine the amount of overfit in the learned model\n",
        "based on these curves? Explain / justify your answer.\n",
        "\n",
        "> An easy way of determining the amount of overfit to a learned model is to compare the learned model on training data with test data. The higher the separation between the two lines, the higher amount of overfitting there is. This is especially true when the training data accuracy is at 100% but test data is far below it. This is because the learned model has hyperfocosued on the training data and has not generalized enough to fit the rest of the test data.\n",
        "\n",
        "Based on the curve in the graph, what size decision tree would you choose to use and why?\n",
        "\n",
        "> Based on the graph, I would choose a tree size of around 10-20 nodes. While this might not have the highest accuracy for training data, it prevents overfitting as you can see any node counts above 20 begin to show a dropoff in accuracy in testing data.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(10 points) To demonstrate your understanding of k-nearest neighbors, construct a labeled dataset where the dimensionality is 1 and the leave-one-out cross-validation accuracy for 1-nearest neighbor is always 0. As a reminder, leave-one-out uses all of the training data except one instance for learning the model and uses the held-out instance for testing, repeating the process for each possible holdout point and averaging the results. Therefore, this describes a situation where the classifier always gets the prediction wrong.\n",
        "\n",
        "> Data: \n",
        "> Classification | feature color\n",
        "> --- | --- \n",
        "> True | Blue\n",
        "> False | Red\n",
        "> False | Red\n",
        "> True | Blue\n",
        "> False | Blue\n",
        "\n",
        "\n",
        "> a situation where it would always predict wrong would be where it trains on all of the top data but tests on the final data point which would always be wrong as it is trained to assume that red is always false and blue is always true.\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(20 points) Consider training a perceptron using the datapoints in the table below, presented in this order.\n",
        "\n",
        "Instance | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8\n",
        "--- | --- | --- | --- | --- | --- | --- | --- | ---\n",
        "Label $y$ | +1 | -1 | +1 | -1 | +1 | -1 | +1 | +1\n",
        "Data $(x_1,x_2)$ | 10, 10 | 0, 0 | 8, 4 | 3, 3 | 4, 8 | 0.5, 0.5 | 4, 3 | 2, 5\n",
        "\n",
        "Given an initial set of weights $w = (1, 1)$ and bias $b=0$, show each step of the perceptron algorithm for the above sequence of instances over one epoch. This includes computation of the activation and adjustment of the weights after each instance.\n",
        "\n",
        "$(x_1)$ | $(x_2)$ | $(w_1)$ | $(w_2)$ | $(b)$ | $(y)$ |  $(a)$ | Correct\n",
        "---|---|---|---|---|---|---| ---\n",
        "10 | 10 | 1 | 1 | 0 | +1 | +20 | [x]\n",
        "0 | 0 | 1 | 1 | 0 | -1 | 0 | [ ]\n",
        "8 | 4 | 1 | 1 | -1 | +1 | +11 | [x]\n",
        "3 | 3 | 1 | 1 | -1 | -1 | +5 | [ ]\n",
        "4 | 8 | -2 | -2 | -2 | +1 | -26 | [ ]\n",
        ".5 | .5 | 2 | 6 | -1 | -1 | +3 | [ ]\n",
        "4 | 3 | 1.5 | 5.5 | -2 | +1 | +20.5 | [x]\n",
        "2 | 5 | 1.5 | 1.5 | -2 | +1 | +8.5| [x]\n",
        "\n",
        "What is the accuracy of the perceptron after this first epoch?\n",
        "\n",
        ">50%\n",
        "\n",
        "Will this perceptron eventually converge on a model with zero error for the training data? Why or why not?\n",
        "\n",
        "> yes, since the data is linerally seperable, the perceptrtion will eventually converge with 0 error\n",
        "\n",
        "---\n",
        "\n",
        "#4\n",
        "\n",
        " \n",
        "For this program, compare the accuracy of four classifiers for correctly classifying the hand-written number from the digits dataset available as a sklearn library (see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html for details). The classifiers are:\n",
        "\n",
        "- Decision tree (you can use the sklearn library for this)\n",
        "- K nearest neighbors with 5 neighbors (you can use the sklearn library for this)\n",
        "- Majority classifier (you can use the sklearn library for this)\n",
        "- Your own implementation of a KNN classifier (do not use the sklearn library for this). This classifier should compute Euclidean distance between pairs of points and take the number of neighbors to consider as a parameter.\n",
        "\n",
        "To report performance, randomly select 2/3 of the data points to use for training and 1/3 to use for testing. Repeat 3 times and report accuracy results averaged over the 3 trials. Compare accuracy results for the classifiers. For your KNN implementation, try different values for $k$ including 1, 3, 5, 7, and 9. Argue which value of $k$ you would choose and why.\n",
        "\n",
        "> I would choose a k value of 3 as it provides the highest accuracy for classifying the data correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZAmhBvZX8nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40461ac-5e67-4c2d-beeb-06373f32ef55"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "from matplotlib import lines\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "# you need to define the min and max values from the data\n",
        "\"\"\" #step_size = 0.05\n",
        "xx, yy, zz = np.meshgrid(np.arange(x_min, x_max, step_size),\n",
        "                         np.arange(y_min, y_max, step_size),\n",
        "                         np.arange(z_min, z_max, step_size))\n",
        "\n",
        "# the colors of the plot (parameter c)\n",
        "# should represent the predicted class value\n",
        "# we found this linewidth to work well\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(xx, yy, zz, c=c_pred, marker='s', edgecolors='k', linewidth=0.2)\n",
        "\n",
        "# you will want to enhance the plot with a legend and axes titles\n",
        "plt.show() \"\"\"\n",
        "\n",
        "def Normalize(X):\n",
        "    min_value = min(X)\n",
        "    range = max(X) - min_value\n",
        "    new_list = [(x-min_value)/range for x in X]\n",
        "    return new_list\n",
        "\n",
        "def EuclideanDistance(X, Y):\n",
        "    return math.sqrt(sum([(a-b) ** 2 for a,b in zip(X,Y)]))\n",
        "\n",
        "\n",
        "# D = Data, Y = labels of D, K = number of nearest neighbors, x is item to classify\n",
        "def KNN_Predict(D, Y, K, x):\n",
        "    S = []\n",
        "    for n in range(D.shape[0]):\n",
        "        S.append((EuclideanDistance(D[n],x), n))\n",
        "    S.sort()\n",
        "    labels = []\n",
        "    for index in range(K):\n",
        "        labels.append(Y[S[index][1]])\n",
        "    return max(set(labels), key = labels.count)\n",
        "\n",
        "\n",
        "def KNN_Group_Predict(D, Y, K, X):\n",
        "    newlabels = []\n",
        "    for point in X:\n",
        "        newlabels.append(KNN_Predict(D, Y, K, point))\n",
        "    return newlabels\n",
        "\n",
        "classifiers = [\n",
        "    (DummyClassifier(strategy=\"most_frequent\"), \"Simple Majority\"),\n",
        "    (DecisionTreeClassifier(criterion=\"entropy\"), \"Decision Tree\"),\n",
        "    (KNeighborsClassifier(n_neighbors=5), \"5NN\"),\n",
        "    (KMeans(n_clusters=5), \"5-Means\")\n",
        "]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    X, y = load_digits(return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "    X.transpose()\n",
        "    for n in range(X.shape[0]):\n",
        "        X[n] = Normalize(X[n])\n",
        "    X.transpose()\n",
        "\n",
        "    X_trainN, X_testN, y_trainN, y_testN = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "\n",
        "    for i in range(1, 10, 2):\n",
        "        newlabels = KNN_Group_Predict(X_trainN, y_trainN, i, X_testN)\n",
        "        print(\"DIY \", i, \"NN Accuracy: \", metrics.accuracy_score(y_testN, newlabels), \"\\n\", sep=\"\")\n",
        "\n",
        "\n",
        "    for clf, name in classifiers:\n",
        "        clf.fit(X_train, y_train)\n",
        "        newlabels = clf.predict(X_test)\n",
        "        print(\"Classifier:\", name)\n",
        "        print(\"Accuracy:\", metrics.accuracy_score(y_test, newlabels), \"\\n\")\n",
        "        #print(metrics.confusion_matrix(y_test, newlabels))\n",
        "        #if (name == \"Decision Tree\"):\n",
        "        #    print(export_text(clf))   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DIY 1NN Accuracy: 0.9882154882154882\n",
            "\n",
            "DIY 3NN Accuracy: 0.98989898989899\n",
            "\n",
            "DIY 5NN Accuracy: 0.9865319865319865\n",
            "\n",
            "DIY 7NN Accuracy: 0.9865319865319865\n",
            "\n",
            "DIY 9NN Accuracy: 0.9848484848484849\n",
            "\n",
            "Classifier: Simple Majority\n",
            "Accuracy: 0.09259259259259259 \n",
            "\n",
            "Classifier: Decision Tree\n",
            "Accuracy: 0.8535353535353535 \n",
            "\n",
            "Classifier: 5NN\n",
            "Accuracy: 0.9915824915824916 \n",
            "\n",
            "Classifier: 5-Means\n",
            "Accuracy: 0.09595959595959595 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}