{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML.H1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eastonco/CS437/blob/main/H1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI8hy0GttCM1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Z3f_sttGtY"
      },
      "source": [
        "# **Homework Assignment #1**\n",
        "\n",
        "Assigned: January 20, 2021\n",
        "\n",
        "Due: February 1, 2021\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of four questions that require a short answer and one that requires you to generate some Python code. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        "(20 points) Use information gain to build a decision tree that predicts the value of the class $Play$ based on the input features $Ace$, $Ten$, and $FirstMove$, using the training data provided below. Show each step of your calculations.\n",
        "\n",
        "Ace | Ten | FirstMove | Play\n",
        "--- | --- | --- | ---\n",
        "false | false | false | stand\n",
        "true | false | true | hit\n",
        "true | true | false | hit\n",
        "true | true | true | stand\n",
        "\n",
        "> Tree:\n",
        "```\n",
        "Ace == True: \n",
        "  Play: Hit\n",
        "Ace == False:\n",
        "  Play: Stand\n",
        "```\n",
        "\n",
        ">Calculations: \n",
        "```\n",
        "entropy 1.0\n",
        "start 1.0\n",
        "value [2, 1] 0.75 0.9182958340544896\n",
        "value [0, 1] 0.25 0.0\n",
        "Ace 0.31127812445913283\n",
        "start 1.0\n",
        "value [1, 1] 0.5 1.0\n",
        "value [1, 1] 0.5 1.0\n",
        "Ten 0.0\n",
        "start 1.0\n",
        "value [1, 1] 0.5 1.0\n",
        "value [1, 1] 0.5 1.0\n",
        "FirstMove 0.0\n",
        "```\n",
        "\n",
        "\n",
        "Is this tree optimal? In other words, does it yield zero classification error on the training data with minimal depth? Explain why or why not. If it is not optimal, draw the optimal tree as well. You can either include the tree(s) as a picture or describe using text.\n",
        "\n",
        ">No this tree is not optimal since it does not classify each action with a 100% accurcy. An optimal tree for this training data would look like this:\n",
        "```\n",
        "Ace == True: \n",
        "  Ten == Firstmove:\n",
        "    Play: Stand\n",
        "  Ten != FirstMove:\n",
        "    Play: Hit\n",
        "Ace == False:\n",
        "  Play: Stand\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#2.\n",
        "\n",
        "(10 points) Express the concept (Play=Hit) learned by all of your trees in Problem 1 as logical if-then rules.\n",
        "\n",
        ">```\n",
        "if Ace == True:\n",
        "  if Ten == FirstMove:\n",
        "    then Play = stand\n",
        "  else if Ten != FirstMove:\n",
        "    Play = hit\n",
        "else:\n",
        "  Play = hit\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(10 points) The decision tree algorithm we discussed in class is a greedy algorithm. What does this\n",
        "mean? What is the reason that most decision tree learning algorithms are greedy?\n",
        "\n",
        "> A greedy algorithm is one that makes the most logical decision at each step of its path rather than looking at the bigger picture and taking a path that may have a better result further down the line. Most decision tree learning algos are greedy becase of the nature of a tree. To traverse it, you have to make decisions immediatly and are unable to look at the end result until you reach it.\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "(10 points) Suppose you are testing a new algorithm on a data set consisting of 100 positive and 100 negative examples. You plan to use leave-one-out cross-validation and compare your algorithm to a baseline function, a simple majority classifier. With leave-one-out cross-validation, you train the algorithm on 199 data points and test it on 1 data point. You repeat the process 400 times, letting each point having a chance to represent the test set, and report the average of the classification accuracies. Given a set of training data, the majority classifier always outputs the class that is in the majority in the training set, regardless of the input. You expect the majority classifier to achieve about 50% classification accuracy, but to your surprise, it scores zero every time. Why?\n",
        "\n",
        "> This could be becase the training data is not sufficint for the test example as it has never been trained on the test example. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#5.\n",
        "\n",
        "(75 points) In this problem you are asked to write a Python program using Google Colab. We provide code below to construct a decision tree using the measures of entropy and gain discussed in class. You need to enhance this program in three ways.\n",
        "\n",
        "- First, note that the provided code assumes that the features are all discrete (there is a finite number of possible feature values) and not continuous. Modify the code to handle either discrete or continuous-valued features. In this case, we ask that you convert continuous-valued features to discrete features using equal-frequency binning, where the number of bins can be a parameter or hard-coded.\n",
        "\n",
        "- Second, use the dataset found at http://eecs.wsu.edu/~cook/ml/alldata.csv to test your model. This a comma-separated data file, one line per data point. The last entry on each line is the class value and the remaining entries represent the feature values. This data is a ``human activity recognition using smartphones'' dataset. The features represent statistical summaries of sensor data collected with a phone-based 3D accelerometer (measuring phone acceleration in X, Y, and Z directions) and a 3D gyroscope (measuring 3-axial angular velocity). The phone was worn by participants while they performed two activities: sit (-1) and stand (1). You will need to read in the dataset and store the data in a structure that the program can process. To test the model, randomly select a subset of points for training the model and randomly select a second subset of points for testing the model. For now, do not worry about whether the subsets overlap or not (in the future, not overlapping will be an important issue).\n",
        "\n",
        "- Third, generate a learning curve to show how the model performs on the data. You can use functions available from the matplotlib library (https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.plot.html) to generate the plot. The y axis of the plot should be classification accuracy, tested on a randomly-selected subset of 100 instances. The x axis of the plot should be the number of instances that was used for training. Create a minimum of 10 points for your learning curve plot with a broad range of x values (you can add more than 10 points to make the curve more complete).\n",
        "\n",
        "Finally, you may notice that the curve jumps around quite a bit. Smooth the curve by repeating the process at least 10 times and plotting the average of the results over the 10 trials.\n",
        "\n",
        "What insights does the learning curve provide on the learning process and the need for a large amount of training data?\n",
        "\n",
        "*Note that all of the code you write needs to be entirely your own, not copied from another existing program or using existing libraries that perform the specified functionality.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLWMwFDeE3nE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b0f1d5b0-be55-40af-b872-75a3e93508a3"
      },
      "source": [
        "# Decision tree learning\n",
        "#\n",
        "# Assumes discrete features. Examples may be inconsistent. Stopping condition for tree\n",
        "# generation is when all examples have the same class, or there are no more features\n",
        "# to split on (in which case, use the majority class). If a split yields no examples\n",
        "# for a particular feature value, then the classification is based on the parent's\n",
        "# majority class.\n",
        "\n",
        "import math\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, majClass):\n",
        "        self.split_feature = -1  # -1 indicates leaf node\n",
        "        self.children = {}  # dictionary of {feature_value: child_tree_node}\n",
        "        self.majority_class = majClass\n",
        "\n",
        "\n",
        "def build_tree(examples):\n",
        "    if not examples:\n",
        "        return None\n",
        "    # collect sets of values for each feature index, based on the examples\n",
        "    features = {}\n",
        "    for feature_index in range(len(examples[0]) - 1):\n",
        "        features[feature_index] = set(\n",
        "            [example[feature_index] for example in examples])\n",
        "    return build_tree_1(examples, features)\n",
        "\n",
        "\n",
        "def build_tree_1(examples, features):\n",
        "    tree_node = TreeNode(majority_class(examples))\n",
        "    # if examples all have same class, then return leaf node predicting this class\n",
        "    if same_class(examples):\n",
        "        return tree_node\n",
        "    # if no more features to split on, then return leaf node predicting majority class\n",
        "    if not features:\n",
        "        return tree_node\n",
        "    # split on best feature and recursively generate children\n",
        "    best_feature_index = best_feature(features, examples)\n",
        "    tree_node.split_feature = best_feature_index\n",
        "    remaining_features = features.copy()\n",
        "    remaining_features.pop(best_feature_index)\n",
        "    for feature_value in features[best_feature_index]:\n",
        "        split_examples = filter_examples(\n",
        "            examples, best_feature_index, feature_value)\n",
        "        if(split_examples):\n",
        "            tree_node.children[feature_value] = build_tree_1(\n",
        "            split_examples, remaining_features)\n",
        "    return tree_node\n",
        "\n",
        "\n",
        "def majority_class(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    return max(set(classes), key=classes.count)\n",
        "\n",
        "\n",
        "def same_class(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    return (len(set(classes)) == 1)\n",
        "\n",
        "\n",
        "def best_feature(features, examples):\n",
        "    # Return index of feature with lowest entropy after split\n",
        "    best_feature_index = -1\n",
        "    best_entropy = 2.0  # max entropy = 1.0\n",
        "    for feature_index in features:\n",
        "        se = split_entropy(feature_index, features, examples)\n",
        "        if se < best_entropy:\n",
        "            best_entropy = se\n",
        "            best_feature_index = feature_index\n",
        "    return best_feature_index\n",
        "\n",
        "\n",
        "def split_entropy(feature_index, features, examples):\n",
        "    # Return weighted sum of entropy of each subset of examples by feature value.\n",
        "    se = 0.0\n",
        "    for feature_value in features[feature_index]:\n",
        "        split_examples = filter_examples(\n",
        "            examples, feature_index, feature_value)\n",
        "        se += (float(len(split_examples)) / float(len(examples))) * \\\n",
        "            entropy(split_examples)\n",
        "    return se\n",
        "\n",
        "\n",
        "def entropy(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    classes_set = set(classes)\n",
        "    class_counts = [classes.count(c) for c in classes_set]\n",
        "    e = 0.0\n",
        "    class_sum = sum(class_counts)\n",
        "    for class_count in class_counts:\n",
        "        if class_count > 0:\n",
        "            class_frac = float(class_count) / float(class_sum)\n",
        "            e += (-1.0) * class_frac * math.log(class_frac, 2.0)\n",
        "    return e\n",
        "\n",
        "\n",
        "def filter_examples(examples, feature_index, feature_value):\n",
        "    # Return subset of examples with given value for given feature index.\n",
        "    return list(filter(lambda example: example[feature_index] == feature_value, examples))\n",
        "\n",
        "\n",
        "def print_tree(tree_node, feature_names, depth=1):\n",
        "    indent_space = depth * \"  \"\n",
        "    if tree_node.split_feature == -1:  # leaf node\n",
        "        print(indent_space + feature_names[-1] +\n",
        "              \": \" + tree_node.majority_class)\n",
        "    else:\n",
        "        for feature_value in tree_node.children:\n",
        "            print(\n",
        "                indent_space + feature_names[tree_node.split_feature] + \" == \" + feature_value)\n",
        "            child_node = tree_node.children[feature_value]\n",
        "            if child_node:\n",
        "                print_tree(child_node, feature_names, depth+1)\n",
        "            else:\n",
        "                # no child node for this value, so use majority class of parent (tree_node)\n",
        "                print(indent_space + \"  \" +\n",
        "                      feature_names[-1] + \": \" + tree_node.majority_class)\n",
        "\n",
        "\n",
        "def classify(tree_node, instance):\n",
        "    if tree_node.split_feature == -1:\n",
        "        return tree_node.majority_class\n",
        "    try:\n",
        "        child_node = tree_node.children[instance[tree_node.split_feature]]\n",
        "    except:\n",
        "        child_node = {}\n",
        "    if child_node:\n",
        "        return classify(child_node, instance)\n",
        "    else:\n",
        "        return tree_node.majority_class\n",
        "\n",
        "\n",
        "def binning(data, bincount, featurecount):\n",
        "\n",
        "    featureList = [[] for i in range(featurecount)]\n",
        "\n",
        "    # manipulating matrix to be by feature, rather than by timestamp\n",
        "    for feature in range(featurecount):\n",
        "        for entry in data:\n",
        "            featureList[feature].append(entry[feature])\n",
        "            #print(\"feature\", feature, \"Entry\", entry[feature])\n",
        "        #print(\"NEW FEATURE\")\n",
        "\n",
        "    # sorting featurelist\n",
        "    for feature in range(len(featureList)):\n",
        "        featureList[feature] = sorted(featureList[feature])\n",
        "\n",
        "    #print(\"SORTED: \", featureList)\n",
        "\n",
        "    size = math.ceil(len(featureList[0]) / bincount)\n",
        "\n",
        "    bins = [[] for i in range(bincount)]  # temp bin for each feature\n",
        "    classification = []\n",
        "\n",
        "    # Binning each feature\n",
        "    for feature in range(featurecount):\n",
        "        index = 0\n",
        "        count = 0\n",
        "\n",
        "        #print(\"SIZEOF\", len(featureList[feature]))\n",
        "        # The actual splitting\n",
        "        for num in featureList[feature]:\n",
        "            bins[index].append(num)\n",
        "            count += 1\n",
        "            if count >= size:\n",
        "                index += 1\n",
        "                count = 0\n",
        "\n",
        "        #print(\"BINS\", bins)\n",
        "\n",
        "        # iterate through original data and repalce with new descrete name\n",
        "        \"\"\"\n",
        "        for index in range(bincount):\n",
        "            label = bins[index][-1]\n",
        "            for entry in data:\n",
        "                if entry[feature] in bins[index]:\n",
        "                    entry[feature] = label\n",
        "        \"\"\"\n",
        "        tmplist = []\n",
        "        for index in range(bincount):\n",
        "            tmplist.append(bins[index][-1])\n",
        "        \n",
        "        tmplist.append(1)\n",
        "\n",
        "        classification.append(tmplist)\n",
        "\n",
        "        bins = [[] for i in range(bincount)]  # reset bins\n",
        "    \n",
        "    print(\"Data Binned...\")\n",
        "    return classification\n",
        "\n",
        "\n",
        "def read_data():\n",
        "    drive.mount('/content/gdrive')\n",
        "    data = np.loadtxt(fname='/content/gdrive/My Drive/437/alldata.csv', delimiter=',')\n",
        "    #data = np.loadtxt(fname='/Users/eastonco/Desktop/alldata.csv', delimiter=',')\n",
        "    mylist = data.tolist()\n",
        "\n",
        "    print(\"Data Read...: \")\n",
        "    return mylist\n",
        "\n",
        "def dataClassification(data, features):\n",
        "\n",
        "    for i, measurements in enumerate(data):\n",
        "        for j, point in enumerate(measurements[:-1]):\n",
        "            for feature in features[j]:\n",
        "                if(feature > point):\n",
        "                    data[i][j] = feature\n",
        "                    break\n",
        "                if feature == features[j][-1]:\n",
        "                    data[i][j] = 1\n",
        "            \n",
        "    print(\"Data Classified...\")\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = read_data()\n",
        "\n",
        "    accuracylist = []\n",
        "    trainingslist = [ 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 250, 300,350, 400, 500, 600, 700,800]\n",
        "\n",
        "    for index, count in enumerate(trainingslist):\n",
        "        print(\"\\nRun #\", index,\" | \", end=\"\")\n",
        "        print(\"Training Count: \" + str(count), end=\"\\n\")\n",
        "        random.shuffle(data)\n",
        "        features = binning(data[100:count], 10, len(data[0])-1)\n",
        "        examples = dataClassification(data[100:count], features)\n",
        "        tests = dataClassification(data[:100], features)\n",
        "\n",
        "        feature_names = list(map(str, range(len(data[0])- 1)))\n",
        "        tree = build_tree(examples)\n",
        "        print(\"Tree Built...\")\n",
        "        #print_tree(tree, feature_names)\n",
        "\n",
        "        positiveCount = 0\n",
        "        negativeCount = 0\n",
        "\n",
        "\n",
        "        for i in range(100):\n",
        "            test_instance = tests[i][:-1]\n",
        "            test_class = classify(tree, test_instance)\n",
        "            if test_class == tests[i][-1]:\n",
        "                positiveCount += 1\n",
        "            else:\n",
        "                negativeCount += 1\n",
        "\n",
        "        print(\"Positive: \", positiveCount)\n",
        "        print(\"Negative: \", negativeCount)\n",
        "        accuracylist.append(positiveCount)\n",
        "    \n",
        "    print(\"AccuracyList: \", accuracylist)\n",
        "    plt.plot(trainingslist, accuracylist, 'bo--')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Data Read...: \n",
            "\n",
            "Run # 0  | Training Count: 110\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  55\n",
            "Negative:  45\n",
            "\n",
            "Run # 1  | Training Count: 120\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  95\n",
            "Negative:  5\n",
            "\n",
            "Run # 2  | Training Count: 130\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  95\n",
            "Negative:  5\n",
            "\n",
            "Run # 3  | Training Count: 140\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  96\n",
            "Negative:  4\n",
            "\n",
            "Run # 4  | Training Count: 150\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  94\n",
            "Negative:  6\n",
            "\n",
            "Run # 5  | Training Count: 160\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  97\n",
            "Negative:  3\n",
            "\n",
            "Run # 6  | Training Count: 170\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  99\n",
            "Negative:  1\n",
            "\n",
            "Run # 7  | Training Count: 180\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  99\n",
            "Negative:  1\n",
            "\n",
            "Run # 8  | Training Count: 190\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  96\n",
            "Negative:  4\n",
            "\n",
            "Run # 9  | Training Count: 200\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  97\n",
            "Negative:  3\n",
            "\n",
            "Run # 10  | Training Count: 250\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  100\n",
            "Negative:  0\n",
            "\n",
            "Run # 11  | Training Count: 300\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  99\n",
            "Negative:  1\n",
            "\n",
            "Run # 12  | Training Count: 350\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  91\n",
            "Negative:  9\n",
            "\n",
            "Run # 13  | Training Count: 400\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  95\n",
            "Negative:  5\n",
            "\n",
            "Run # 14  | Training Count: 500\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  98\n",
            "Negative:  2\n",
            "\n",
            "Run # 15  | Training Count: 600\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  98\n",
            "Negative:  2\n",
            "\n",
            "Run # 16  | Training Count: 700\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  98\n",
            "Negative:  2\n",
            "\n",
            "Run # 17  | Training Count: 800\n",
            "Data Binned...\n",
            "Data Classified...\n",
            "Data Classified...\n",
            "Tree Built...\n",
            "Positive:  100\n",
            "Negative:  0\n",
            "AccuracyList:  [55, 95, 95, 96, 94, 97, 99, 99, 96, 97, 100, 99, 91, 95, 98, 98, 98, 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAdYklEQVR4nO3deZRV1Zn38e9TImBhmLEEFTCGVqMdhWI5x3ZFbZUYTN42Nr6VFtNRnLCDmk6bOOR1SrQdohlESUxClGCcjSRqKxqNpoVVGKOIIqhMSkmB4gBaBcXz/rFPeWue7nCG+n3Wuuucu8+pe55bde5T++69zz7m7oiISLaUxR2AiIgUnpK7iEgGKbmLiGSQkruISAYpuYuIZFCfuAMAGD58uI8dOzbuMEREUmXRokXr3X1EW9sSkdzHjh1LdXV13GGIiKSKma1sb5uaZUREMkjJXUQkg5TcRUQySMldRCSDlNxFRDKo0+RuZr8ys3VmtrhJ2VAze8zMlkXLIVG5mdlPzGy5mb1oZhOKGXxvNGcOjB0LZWVhOWdO3BGJSE8U+7PclZr7b4BjW5RdCMx393HA/Og5wHHAuOgxDZhZmDAFwh9/2jRYuRLcw3LaNCV4kbQpxWfZujLlr5mNBea5+77R86XAEe6+1sxGAn929z3N7NZofW7L/Tp6/YkTJ7rGuXdu7NhwErQ0ZgysWFHqaESkpwr1WTazRe4+sa1tPW1zr2iSsGuAimh9F2B1k/3WRGVtBTXNzKrNrLq2traHYfQuq1a1Xb5yJXzjG3DXXaWNR0Q69/bb8NBDcNllMHkyLFjQ/me5vfKeyPsKVXd3M+v2HT/cfRYwC0LNPd84smzzZvjhD9vfvsMO8OSTMGQInHQSbN0KEyfCPvtAZWVYHz8ePvOZ0sUs0tu4h0RuBqNGwdKlcMQRUFMTtpvBXnvBhg0wenTbNffRowsXT09r7u9EzTFEy3VR+VvAbk322zUqS7WWHR9nn12aTk13uO8+2HtvuOoqOOSQkMibKi+HX/wC3noLbrghlG3cGL7ePfUUXHAB/NM/waBB8POfh+0ffgh//jN88EH771Ht+CIda2iABx+ESy+FL38ZRo6EXXeFG28M23fbDY4+Ojz/y1/C523JEpg0KXyey8ubv155eSgvGHfv9AGMBRY3eX4tcGG0fiHw39H6l4GHAQMOAhZ25fUrKys9qe64w7283D2k2rYf5eVhv0JqaHA//vjw+v/4j+5PP52LZ8wYd7Ow7Oy4NTXuf/yj++WXu1dXh7JHH83FPm6c+0EHuW+/ffHfk0gabdvmvnq1+/33u198sfsNN+TKhwxxLytz32cf91NOcb/pJvcXX+za63b3s9wWoNrbyauddqia2VzgCGA48A7wA+AB4C5gNLASOMnd3zUzA35GGF2zGfimu3faU5rkDtX2Oj5aKlSnZl0d9OsX1q+4AgYOhHPOgT4FnOLt/ffhr3+FRYvC46GHQi2kJXXUSm/jDu++C8OGhefnnAP33AProraJsjI44YTwjRrg5ZdDjhgwIJZwO+xQ7dJomWJLcnIvKwt/8M6YwbZtPT+OeziJzj8ffvWr8HWuVNp7j/m+J5Gke/tteO65XEXn+efDOV9bG87/Sy6BNWtC31VlJey3X+vmlDgVY7RMr7FLm2N9WsunI+SVV0IyP+mkUGMYNKjnr9UT7cVeyM6d3i5tfRppixc6jtk9fAu99174/vfh449D+Y9/DP/yL3DNNbB2LRx/fBjVsnVr2H7FFfDrX8P06XDwwclK7J1qr72mlI8kt7lPntxxe3u+7dNXXunep4/74MHuP/uZ+5YthY2/K9rqVygrc7/99tLHkkVt/X6T3KeRtnjd24/5kkvcjz7afdiwXHmfPu5//3v4uddec3/uOffNm+ONv6fIp829FJLaLLN+Pey+O+y7b/ivvmpVqM1OmgR/+lN4vvPOcO21UFXV9ddtPM3KyuDWW2HhQvjRj2CnnYr3XjozZw5cdFF4T0OHhuFaN98MZ50VX0xZMWZM2+OXG/s0rrgCnnmm+baKCvjtb8P6978fmgxa/uysWWF9xozw7a+pvffOjdo444zWfScTJoRzDuCUU+Cdd3Lbnn4aPvmkdbz9+8Phh4f1o4+G73wnrB9/PGzZ0nzfyZNDe/WWLWF7S//6r/Dv/x5GkHz96623n3oqnHxyGEY4dWrr7WefHdq+V6wI76+9mCsqwrDEysrwnisr4QtfCO8lCzpqlknEnZiSasAAuPxyOOYY+PznC/OaL78M554LU6aEy43POCM84lZVlfsH5R4+vBdeGD5Ao0bFG1saffRROH/MOr9g5eOPmw9LheZf/zdvbr1906aub//oo9bbN2/OrX/4YfPtbSXJxvLG/RqbNSCUtUzuLbe3VFcXlu5tb6+v79r2hoawvb2Y163LjTPvddqr0pfykaRmme4OT6qudj/zzNbNKS1f5xe/cD//fPfttgvDp2bPLk78hbJsmfsXv+i+dGnckaTHihXuP/2p+zHHuPftm/vqv/PObTfnjRkTa7jtGjMmXfG6pzPmQqCDZpnYE7snKLn3pK3xnnvCfvPnd/w6jY/TT3evrS3+e5HSWbLEfd99c3/jf/iH8I/8jTfC9rS1YactXvd0xlwISu5d1JP//ps2uQ8Y4H7GGZ2/zs47F/kNFEFtrft557l/+GHckSTD+++73323+9SpoZbuHn43Rx3lfv317X/TKcQFK6WUtnjd0xlzvjpK7upQbaKn472nTIEnnghjZvv0yda48b/+FQ49NIy/v/76uKOJz623husQnnoqtC8PGRI6Mi+9NO7IpDfTOPcu6ul475NOChc9PPVUeF5R0fZ+aRw3fsghocP3xhvDBR69wdatYfTKT3+aK3vggXAxy4wZYWTGunVK7JJw7VXpS/koZbNMR1/d7rjDvX//7rfbbd7sPmqU+/Dh4XVLNf9Mqbz3nntFhXtlpfvWrXFHUxzvved+553uVVXuQ4eGv1nfvqHcPTS/iSQNHTTL9Kqae2d3P6mqCmO9+/ULTSiNY4k7G8N+331hJsb163PNMdtvH6427c7rJNXgwXDTTWGsdePMklmwbFmYZwfgd78LzWuPPhrGZd91V6idDx4ctqfqykQRetncMsW6k1FvuEOSO1x9dbjgpatTMiTNli3w7LMwb16YLO2118I8Pt/8ZmhWW7YMDjwQttsu7khFukYTh0U66+h8992wvXFGuEK9bta4h/eWBtu2hb/Pxo3hauONG6Fv33ATha98Bb72tfT+sxLpNR2qHd1UY+ed25/dsbGj85ZbwoT7GzZ077i9aeKt2lr40pfg/vvjjaO9SaLcw6X4114bblIyZUooHzw4XA5/772h+ezRR8NkUErsklntNcaX8lGIDtWu3FTDLHSStdfRud9+7occUphjp7kDtSP19eH3tMsuYcx3HNr7fVdVue+xR65sv/3cf/jDeGIUKQV6w0VM7V041PIxbFhutMxOO+U+/EuXhu033tiz4/emCygWLAjv89xz4zl+e3/rQYPcJ01yv/lm91Wr4olNpJQ6Su6ZaXPv7k01Nm8OHZ6VlfDII+HehRdfDKtXh/sgSsfOPTeMnHnuOTjggNIeu7f1cYi0p1e0uXe1fbtxv/Jy+M//DG2v//u/YejboYcqsXfVVVeF2SKvvLK0x62ra/+WZlns4xDpqcwk97buJt5Sy7uLn3MO7Lhj6CB86SV4/fV03HEmCQYOhD/+MYwPL6Xp08MUti3vKVvwO8eLpFxmkntVVZj7ZODA3IVDZ50Vlu1dSPTAA6Em+Mkn4Wt+TU3zi5qkY/vtF/451tWFUTTF1NgM873vwd13w29+0/HfVqS3y0ybO8Crr4Y70MydmxsC15HecPFRsW3bFi78GTEi1OQLPf7dPUza9Ze/wB13pGd8vUgp9Io2d8jd3aVv367t39kdcqRzZWXwjW/Aww+HGnUhffIJnH56+Ab23nvN7+4jIh3LVHJvvPVWV5N7b7r4qJimTw+jjr797XAFaCGsWRMuQrrttjDfz0MPaX4Xke7IZHLv169r+7fVCauOue7bbrvQ5r1uXbjvar62bYNjj4UlS8KkbFdeqfleRLorUzfI7m7NvbED7qKLQlPM6NEhsatjrvsmTAhznT/zTGge6+o/2KYau3/KymDmTBg+PPShiEj3ZapD1T3caKGsTDW9OHzySZjquCe/+48/hjPPhM99Di65pPCxiWRRr+lQNet5cpH89e8ffvfr14dpdbtq1So47DD47W+7dpWxiHQuU8m9ujpcmFRTE3ckvdt3vwsnngjLl3e+75NPhs7Y5cvhwQd16zqRQslUcn/1Vbj5Zvjww7gj6d2uvDL0e5x1Vsc18ZoamDQptK0vXAiTJ5cuRpGsy1Ry726HqhTHqFHwox/B44+3fbVvQ0NY7rxzmF99wQLYc8/SxiiSdZlM7j0ZqSGFdeaZ4crV885rfvOTFSvCLJIPPBCeT5oUpowQkcLKVHLv7hWqUjyNY98/+1nYf/8wgqmiAvbdN0zQpn/AIsWVqXHu7iFpKLknw0svweLFYe58CBc5mcF118Fxx8Ubm0jWZWqcuySLJmYTKa5eM85dkkUTs4nEJ6/kbmbfNrPFZvaymc2Iyoaa2WNmtixaDilMqJ2bOxdOO61UR5POaGI2kfj0OLmb2b7A6cABwH7A8Wb2OeBCYL67jwPmR89LYuHCwk87Kz2nidlE4pNPzX1vYIG7b3b3rcBTwP8BTgBmR/vMBr6aX4hdV1+vztQkqaoKI2Z0xySR0stntMxi4CozGwZ8DEwCqoEKd18b7VMDVLT1w2Y2DZgGMLpA39Pr6pTck6aqSslcJA49rrm7+yvANcD/AI8ALwANLfZxoM3hOO4+y90nuvvEESNG9DSMZurrNX5aRATy7FB199vcvdLdDwfeA14D3jGzkQDRcl3+YXZNeTnstFOpjiYiklx5XcRkZju5+zozG01obz8I2B2YClwdLR/MO8ouuuWWUh1JRCTZ8r1C9d6ozX0LcI67bzSzq4G7zOxbwErgpHyDFBGR7skrubv7F9so2wAcmc/r9tTFF4cpCDTUTkR6u0zNLfPUU+FOTCIivV2mph/QOHcRkSBTyb2uTkMhRUQgY8ldNXcRkSBTyX3UKNhll7ijEBGJX6Y6VB9/PO4IRESSIVM1dxERCTKV3L/yFbj55rijEBGJX6aS+/z58OabcUchIhK/TCV3zQopIhJkJrk3NISHhkKKiGQoudfXh6Vq7iIiGUruDQ0wfjyMHBl3JCIi8cvMOPcdd4Tnn487ChGRZMhMzV1ERHIyk9xXr4bKSnj44bgjERGJX2aS+0cfhWaZDz6IOxIRkfhlJrnX1YWlhkKKiGQouTcOhVRyFxHJUHJvrLlrnLuISIaS+447wuGHw7BhcUciIhK/zIxzHz8+3CBbREQyVHMXEZGczCT3P/0J9toLXn897khEROKXmeS+YQMsXQrucUciIhK/zCR3DYUUEcnJTHLXUEgRkZzMJHfV3EVEcjKT3MeMgUmTYIcd4o5ERCR+mRnn/rWvhYeIiGSo5i4iIjmZSe5XXAF77BF3FCIiyZCZ5L5hA6xfH3cUIiLJkJnkXl+vYZAiIo0yk9zr6jQMUkSkUV7J3czOM7OXzWyxmc01s/5mtruZLTCz5Wb2ezMrScpVzV1EJKfHyd3MdgH+A5jo7vsC2wFTgGuAH7v754D3gG8VItDOHHggTJ5ciiOJiCRfvuPc+wA7mNkWoBxYC3wJ+L/R9tnA/wNm5nmcTk2fXuwjiIikR49r7u7+FnAdsIqQ1N8HFgEb3X1rtNsaYJe2ft7MpplZtZlV19bW9jQMERFpQz7NMkOAE4DdgVHAAODYrv68u89y94nuPnHEiBE9DeNTkybBMcfk/TIiIpmQT4fqUcCb7l7r7luA+4BDgcFm1tjcsyvwVp4xdsmHH8LWrZ3vJyLSG+ST3FcBB5lZuZkZcCSwBHgSODHaZyrwYH4hdo2GQoqI5OTT5r4AuAd4Hngpeq1ZwH8B55vZcmAYcFsB4uyUhkKKiOTkNVrG3X8A/KBF8RvAAfm8bk+o5i4ikpOZKX+//nXYbbe4oxARSYbMJPfLL487AhGR5MjM3DINDXFHICKSHJlJ7sOGwYwZcUchIpIMmUnu6lAVEcnJTHKvr1dyFxFplInkvnUrbNumce4iIo0ykdzr68NSNXcRkSATyb2sDL7znTCnu4iIZGSce//+cO21cUchIpIcmai5NzTAxo2aFVJEpFEmkvuKFTBkCNx5Z9yRiIgkQyaSe11dWKpDVUQkyERy12gZEZHmMpHcG2vuGucuIhJkIrmr5i4i0lwmkvtuu8Fll8Eee8QdiYhIMmRinPvYsXDppXFHISKSHJmouW/aBGvWwJYtcUciIpIMmUju8+aFppnly+OOREQkGTKR3NWhKiLSXCaSu4ZCiog0l4nkrpq7iEhzmUjumn5ARKS5TCT3ww+H66+H8vK4IxERSYZMjHOvrAwPEREJMlFzr6mBpUvjjkJEJDkykdyvvx7Gj487ChGR5MhEcq+v1zBIEZGmMpHc6+o0UkZEpKlMJPf6eiV3EZGmMpPc1SwjIpKTiaGQp50GkyfHHYWISHJkIrkfcUTcEYiIJEsmmmWWLIFXXok7ChGR5OhxcjezPc3shSaPD8xshpkNNbPHzGxZtBxSyIDbMn06nHFGsY8iIpIePU7u7r7U3fd39/2BSmAzcD9wITDf3ccB86PnRaWhkCIizRWqWeZI4HV3XwmcAMyOymcDXy3QMdqloZAiIs0VKrlPAeZG6xXuvjZarwEq2voBM5tmZtVmVl1bW5vXwTUUUkSkubyTu5n1BSYDd7fc5u4OeFs/5+6z3H2iu08cMWJEXjGoWUZEpLlCDIU8Dnje3d+Jnr9jZiPdfa2ZjQTWFeAYHbrxRhg4sNhHERFJj0I0y5xMrkkG4A/A1Gh9KvBgAY7RoWOPhUMOKfZRRETSI6/kbmYDgKOB+5oUXw0cbWbLgKOi50X15JPw2mvFPoqISHrkldzdfZO7D3P395uUbXD3I919nLsf5e7v5h9mx044AWbOLPZRRETSIxNXqGoopIhIc6lP7u4aCiki0lLqk/vWrSHBq+YuIpKT+uReXx+WSu4iIjmpn/K3Xz945BEYNy7uSEREkiP1yb1PHzjmmLijEBFJltQ3y2zaBPfeC6tWxR2JiEhypD65v/02nHgiPP103JGIiCRH6pN7Y4eqhkKKiOSkPrnX1YWlRsuIiOSkPrmr5i4i0lpmkrtq7iIiOakfCrn//vDMM7DPPnFHIiKSHKlP7gMHwqGHxh2FiEiypL5Z5s03YfZs2Lgx7khERJIj9cl9wQI49VSoqYk7EhGR5Eh9ctdQSBGR1lKf3DUUUkSktdQnd9XcRURaS31yV81dRKS11Cf3U06BF16AAQPijkREJDlSP859+PDwEBGRnNTX3J99Fm65Je4oRESSJfXJ/f774YIL4o5CRCRZUp/c6+vVmSoi0lLqk3tdnYZBioi0lPrkrpq7iEhrqU/uqrmLiLSW+qGQP/kJfPxx3FGIiCRL6pO7xriLiLSW+maZOXPg9tvjjkJEJFlSX3P/5S+hoQH+7d/ijkREJDlSX3Ovq9NoGRGRllKf3OvrNVpGRKSl1Cd31dxFRFrLK7mb2WAzu8fMXjWzV8zsYDMbamaPmdmyaDmkUMG2RTV3EZHW8q253wQ84u57AfsBrwAXAvPdfRwwP3peNAsXwqxZxTyCiEj69Di5m9kg4HDgNgB3r3f3jcAJwOxot9nAV/MNsiODBsHAgcU8gohI+uRTc98dqAV+bWZ/M7NfmtkAoMLd10b71AAVbf2wmU0zs2ozq66tre1xEJddBvPm9fjHRUQyKZ/k3geYAMx09/HAJlo0wbi7A97WD7v7LHef6O4TR4wY0eMgrrsOnniixz8uIpJJ+ST3NcAad18QPb+HkOzfMbORANFyXX4hdkwdqiIirfU4ubt7DbDazPaMio4ElgB/AKZGZVOBB/OKsMMYNOWviEhb8p1+4Fxgjpn1Bd4Avkn4h3GXmX0LWAmclOcx2rVlS1iq5i4i0lxeyd3dXwAmtrHpyHxet6vq68NSNXcRkeZSPXHYgAG52ruIiOSkOrmbQZ9UvwMRkeJI9dwy69fD2WfDggWd7ysi0pukOrm/+y7MnAnLl8cdiYhIsqQ6uatDVUSkbalO7nV1YamhkCIizaU6uavmLiLStlQn94aGkNiV3EVEmkv1QMLDDoNPPok7ChGR5El1zV1ERNqW6uS+YAGccgqsWRN3JCIiyZLq5P7GG3D77fDRR3FHIiKSLKlO7o1DIdWhKiLSXGqT+5w5cMEFYf2LXwzPRUQkSOVomTlzYNo02Lw5PH/rrfAcoKoqvrhERJIilTX3iy7KJfZGmzeHchERSWlyX7Wqe+UiIr1NKpP76NHdKxcR6W1SmdyvugrKy5uXlZeHchERSWlyr6qCWbNgzJhwN6YxY8JzdaaKiASpHC0DIZErmYuItC2VNXcREemYkruISAYpuYuIZJCSu4hIBim5i4hkkLl73DFgZrXAyi7uPhxYX8RwCk3xFpfiLa60xQvpizmfeMe4+4i2NiQiuXeHmVW7+8S44+gqxVtcire40hYvpC/mYsWrZhkRkQxSchcRyaA0JvdZcQfQTYq3uBRvcaUtXkhfzEWJN3Vt7iIi0rk01txFRKQTSu4iIhmUuORuZr8ys3VmtrhJ2VAze8zMlkXLIVG5mdlPzGy5mb1oZhNKHOtuZvakmS0xs5fN7NsJj7e/mS00s79H8V4Wle9uZguiuH5vZn2j8n7R8+XR9rGljLdJ3NuZ2d/MbF5K4l1hZi+Z2QtmVh2VJfKciGIYbGb3mNmrZvaKmR2c1HjNbM/o99r4+MDMZiQ13iiG86LP22Izmxt9Dot/Drt7oh7A4cAEYHGTsv8GLozWLwSuidYnAQ8DBhwELChxrCOBCdH6Z4DXgM8nOF4DdozWtwcWRHHcBUyJym8BzorWzwZuidanAL+P6Zw4H/gdMC96nvR4VwDDW5Ql8pyIYpgNnBat9wUGJzneJnFvB9QAY5IaL7AL8CawQ5Nz99RSnMOx/FG68AsZS/PkvhQYGa2PBJZG67cCJ7e1X0xxPwgcnYZ4gXLgeeBAwtVxfaLyg4FHo/VHgYOj9T7RflbiOHcF5gNfAuZFH9LExhsdewWtk3sizwlgUJR8rEV5IuNtEeM/A88mOV5Ccl8NDI3OyXnAMaU4hxPXLNOOCndfG63XABXReuMvrtGaqKzkoq9P4wm14cTGGzVxvACsAx4DXgc2uvvWNmL6NN5o+/vAsFLGC9wIfBfYFj0fRrLjBXDgf8xskZlNi8qSek7sDtQCv46avn5pZgNIbrxNTQHmRuuJjNfd3wKuA1YBawnn5CJKcA6nJbl/ysO/tESN3zSzHYF7gRnu/kHTbUmL190b3H1/Qo34AGCvmENql5kdD6xz90Vxx9JNh7n7BOA44BwzO7zpxoSdE30IzaAz3X08sInQrPGphMULQNRGPRm4u+W2JMUbtf2fQPgnOgoYABxbimOnJbm/Y2YjAaLluqj8LWC3JvvtGpWVjJltT0jsc9z9vqg4sfE2cveNwJOEr4SDzazxlotNY/o03mj7IGBDCcM8FJhsZiuAOwlNMzclOF7g09oa7r4OuJ/wTzSp58QaYI27L4ie30NI9kmNt9FxwPPu/k70PKnxHgW86e617r4FuI9wXhf9HE5Lcv8DMDVan0po224sPyXqET8IeL/JV7OiMzMDbgNecfcbUhDvCDMbHK3vQOgfeIWQ5E9sJ97G93Ei8ERUKyoJd/+eu+/q7mMJX8GfcPeqpMYLYGYDzOwzjeuEduHFJPSccPcaYLWZ7RkVHQksSWq8TZxMrkmmMa4kxrsKOMjMyqN80fj7Lf45XMoOkC52QMwltE1tIdQqvkVoc5oPLAMeB4ZG+xrwc0K78UvAxBLHehjh69+LwAvRY1KC4/0C8Lco3sXApVH5Z4GFwHLC19x+UXn/6PnyaPtnYzwvjiA3Wiax8Uax/T16vAxcFJUn8pyIYtgfqI7OiweAIQmPdwChNjuoSVmS470MeDX6zN0O9CvFOazpB0REMigtzTIiItINSu4iIhmk5C4ikkFK7iIiGaTkLiKSQUruIiIZpOQuIpJB/x9vzryyeQ8VJAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MybXjx2MtGDx"
      },
      "source": [
        ""
      ]
    }
  ]
}